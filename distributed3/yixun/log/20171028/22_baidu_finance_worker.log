2017-10-28 22:01:19,384  [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: yixun)
2017-10-28 22:01:19,384  [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'yixun', 'CONCURRENT_REQUESTS': 1000, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_TIMEOUT': 50, 'LOG_FILE': 'log/20171028/22_baidu_finance_worker.log', 'LOG_FORMAT': '%(asctime)s,%(msecs)d  [%(name)s] %(levelname)s: %(message)s', 'NEWSPIDER_MODULE': 'yixun.spiders', 'RETRY_HTTP_CODES': [500, 502, 503, 504, 400, 408, 404, 403], 'RETRY_TIMES': 5, 'SPIDER_MODULES': ['yixun.spiders']}
2017-10-28 22:01:19,428  [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-10-28 22:01:19,882  [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'yixun.middlewares.ChoiceAgent',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'yixun.middlewares.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-10-28 22:01:19,884  [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'yixun.middlewares.ProcessResponseMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-10-28 22:01:19,885  [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-10-28 22:01:19,885  [scrapy.core.engine] INFO: Spider opened
2017-10-28 22:01:19,894  [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-10-28 22:01:19,894  [baidu_finance] INFO: Spider opened: baidu_finance
2017-10-28 22:01:19,896  [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024
2017-10-28 22:01:19,904  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  'dict' object has no attribute 'to_string'
<GET http://news.baidu.com/finance>
2017-10-28 22:01:20,4  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.AttributeError: 'dict' object has no attribute 'to_string'>
2017-10-28 22:01:20,104  [scrapy.core.engine] INFO: Closing spider (finished)
2017-10-28 22:01:20,106  [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/builtins.AttributeError': 1,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 10, 28, 14, 1, 20, 105358),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 2,
 'log_count/INFO': 8,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 10, 28, 14, 1, 19, 894346)}
2017-10-28 22:01:20,106  [scrapy.core.engine] INFO: Spider closed (finished)
2017-10-28 22:14:54,351  [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: yixun)
2017-10-28 22:14:54,352  [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'yixun', 'CONCURRENT_REQUESTS': 1000, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_TIMEOUT': 50, 'LOG_FILE': 'log/20171028/22_baidu_finance_worker.log', 'LOG_FORMAT': '%(asctime)s,%(msecs)d  [%(name)s] %(levelname)s: %(message)s', 'NEWSPIDER_MODULE': 'yixun.spiders', 'RETRY_HTTP_CODES': [500, 502, 503, 504, 400, 408, 404, 403], 'RETRY_TIMES': 5, 'SPIDER_MODULES': ['yixun.spiders']}
2017-10-28 22:14:54,398  [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-10-28 22:14:54,875  [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'yixun.middlewares.ChoiceAgent',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'yixun.middlewares.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-10-28 22:14:54,878  [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'yixun.middlewares.ProcessResponseMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-10-28 22:14:54,879  [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-10-28 22:14:54,879  [scrapy.core.engine] INFO: Spider opened
2017-10-28 22:14:54,888  [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-10-28 22:14:54,889  [baidu_finance] INFO: Spider opened: baidu_finance
2017-10-28 22:14:54,890  [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024
2017-10-28 22:14:54,984  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.baidu.com/finance> (referer: None)
2017-10-28 22:14:55,141  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200850605_313745> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,167  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200851917_162818> from <GET http://business.sohu.com/20171028/n520449743.shtml>
2017-10-28 22:14:55,208  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dzwww.com/xinwen/shehuixinwen/201710/t20171028_16585105.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,250  [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.sohu.com/a/200850605_313745> (referer: http://news.baidu.com/finance)
Traceback (most recent call last):
  File "E:\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python_code\Spider3\distributed\yixun\yixun\spiders\baidu_finance.py", line 188, in parse_base
    item['title'] = response.xpath('//div[@class="text-title"]/h1/text()').extract()[0]
IndexError: list index out of range
2017-10-28 22:14:55,256  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://news.qq.com/a/20171028/027677.htm> from <GET http://news.qq.com/a/20171028/027677.htm>
2017-10-28 22:14:55,271  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.ifeng.com/a/20171028/52834010_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,280  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.cecb2b.com/info/20171028/3613657.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,292  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.ifeng.com/a/20171028/15751804_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,302  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biz.jrj.com.cn/2017/10/28191423299779.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,308  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://industry.people.com.cn/n1/2017/1028/c413883-29614276.html> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,317  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.chinabyte.com/265/14329265.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,332  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362753.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,364  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/cj/2017/10-28/8362769.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,474  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350864> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,536  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28205123299849.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:55,638  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72411: illegal multibyte sequence>
2017-10-28 22:14:57,442  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362715.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:57,608  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200854533_143019> from <GET http://business.sohu.com/20171028/n520452220.shtml>
2017-10-28 22:14:57,759  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28211923299869.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:57,856  [scrapy.core.engine] DEBUG: Crawled (200) <GET https://news.qq.com/a/20171028/027677.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:57,860  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70115: illegal multibyte sequence>
2017-10-28 22:14:57,995  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/ty/shipin/cns-d/2017/10-28/news738503.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:14:58,89  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350881> (referer: http://news.baidu.com/finance)
2017-10-28 22:15:00,36  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28212023299870.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:15:00,137  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 69924: illegal multibyte sequence>
2017-10-28 22:15:03,628  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200851917_162818> (referer: http://news.baidu.com/finance)
2017-10-28 22:15:03,739  [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.sohu.com/a/200851917_162818>
{'about': 'from baidu',
 'author': '',
 'content': '随着深圳终于后知后觉地袭来阵阵凉意，2017年也即将敲响结束的钟声。作为一名周期行业的研究员，笔者在今年也是见证了熄火许久的周期行业都各自迎来自己的大牛市，从造纸、钢铁、水泥、煤炭到有色，相信大多数追随周期的投资者都已经获得了不错的收益。前几日，笔者听闻一个朋友，他在年中的时候买入江西铜业（00358.HK），理由是看好铜价的走势，所以买一点铜业的龙头。结果是，铜价今年确实也走的不错，LME铜上周刚刚突破7000美元大关，刷新近三年的新高（全年走势见上图）。铜价今年的回暖涨幅，也并不比相似属性的铝或者钢材差太多。尤其是LME铜，年初至今累计涨幅将近27%，并且发力主要集中在下半年，近两个月累计涨幅约10%。但是，相对于其它周期股，铜股的走势也就只能用“虚弱”来形容了，朋友表示十分郁闷，好像商品的价格对股价影响并不明显。港股的铜概念股代表，还有一家紫金矿业（02899.HK），涨幅同样很微弱。再看看周期其他行业的龙头，随着商品价格的上涨，股价也基本一起“升天”了。那么，问题就来了。面对亮眼的铜价，以及持续平庸的股价，铜股还值得我们继续坚守吗？要回答这个问题，首先需要明确铜股股价不太涨的原因。笔者认为，导致的原因可能主要有两点。▌一、原因一：市场对铜价进一步上涨缺乏自信股市是经济的晴雨表，股票价格反映的也是市场对未来价值的预期。铜价上涨，而股价不涨，很大可能就是市场对后市判断缺少基准，拿不准铜价未来的走势，所以对铜价进一步上涨的空间缺乏自信，保持谨慎态度。前面提到的，今年股价势如破竹的板块，其实大多数都有两个相同的特征。特征一是企业绝大多数产能都集中在国内，例如电解铝，例如钢材，再例如水泥等等。特征二则是行业都处在国家主动的调控下，例如供给侧改革，例如环保督查关闭不合规小厂，再例如北方采暖季停工等等，这些政策的最终导向，都是很明确的产能减少。并且，今年的政策力度还非常大。这些动作，都给了明确的“强心剂”，让市场对行业的后市充满信心。但是对于铜来说，则不太具备以上两个特征。中国是世界上最大的铜消费国，但是铜的储量和基础储量分别仅占世界总量的5.53%和6.67%，对于铜精矿的需求，需要依靠大量从智利、秘鲁、澳洲等地进口来满足。从2003年到2014年，中国铜精矿进口量，从266.7万吨增加到1182万吨。由于产能不在国内，也就沾不到国内大力度去产能的利好，市场对于后市的判断比较模糊，也就能够解释。▌二、原因二：铜企的盈利尚未释放铜价上升通道开启的比较晚，基本是在七月底。而商品价格变化带给公司的盈利，很多并不会立刻释放。虽然有客观的原因导致铜股的股价走势并不理想。但是笔者认为，以上两个原因，对铜企股价释放的阻力，未来有机会逐渐减小。▌三、铜价走势已经基本明朗化首先，铜价未来的走势已经从模糊步入明朗化。今年铜价的上涨，主要是由于商品库存的下降以及全球经济的企稳。目前，全球的货币政策仍趋紧，这意味着全球经济比预期要更强劲。铜作为基本金属，其价格很大程度上跟经济挂钩。所以从大体经济上来看，铜基本可以保持一个长牛的走势。10月24日，高盛也将未来１２个月的LME铜目标价上调至７０５０美元，该行预计随着经济的修复，明年铜的供应将会短缺１３万吨。1.供给端铜精矿加工费（TC/RC），是将铜精矿转化为精铜的处理和精炼费用，即矿产商和贸易商向冶炼厂支付的费用。TC/RC可以很好的反映铜精矿的供给，因为决定加工费高低的因素，就是铜精矿的供求关系。当铜精矿供给短缺时，矿产主对冶炼厂商的谈判占据主动，其支付的费用就会降低。反之，费用则上升。截至2017年9月，TC/RC费用已经从2016年11月的101.5/10.15美元/吨，降低到了83/8.3美元/吨，这正面说明了铜精矿供给量正在下降的事实。查阅数据，上半年，全球的矿山产量只有1月份同比增加了，2-6月份同比分别下滑5%、7%、0%、2%和2%。且接下来，铜精矿的供给仍然会受到限制。出口受限：据悉，刚果（金）方面要求华刚矿业在国内将铜和钴加工成金属再出口，不允许出口未经加工的铜和钴。今年上半年，华钢矿业供给出口了11.5万吨铜精矿，年产量为23万吨，计划产能40万吨。华钢矿业出口的铜精矿量虽不多，但是目前全球的铜供给处于紧平衡或者存在缺口，该限制，可能进一步恶化铜精矿的供给情况。劳工协议到期：今年10月20日到11月30日，据统计有约92万吨产能的劳工协议到期，且大部分位于智利，有可能再度引发上半年的劳工罢工的事件。短期新增产能不多：一般来说，铜矿山从资本支出到正式投产，需要4-5年的时间，投产周期较长。2011年，全球30家主要铜矿公司支出资本达到峰值，之后支出明显下滑，新增的铜矿产能也相应地在2015-2016年达到峰值。最近一波的全球铜矿产能投资，基本是集中于2016年铜价回暖之后，所以产能高峰产出算一算也需要在2022年左右才会到来。据悉全球十大铜企，只有力拓和必和必拓在2020年前有新增产能投入，其余公司均没有计划。2.需求端中国基础设施建设：截至2017年8月，我国基础设施建设投资和公共财政支出分别同比增长16.09%和13.08%。增速虽稍有放缓，但是依然维持在两位数。美国基建：特朗普表示，未来五年对基建投资将增加5500亿美元。如若顺利实施，美国2017-2020每年基建投资的增长率将达到5.7%。根据高盛的预测，特朗普每年约有1万亿美元的基建项目，在未来的10年内将推动美国铜消费增速上升至4%。“一带一路”：沿线国家的建设薄弱，也有望成为拉动铜需求的新亮点。国内铜需求平稳：我国铜终端的需求主要是集中在电力、家电（空调为主）、交通和建筑领域。截止8月份，电网基本建设投资累计完成额同比增长7.88%，且“十三五”规划电网投资3.34万亿，是“十二五”2万亿规划的1.67倍，电网建设的投资会加倍增长；截止8月份，我国汽车产量累计增长5.9%，由于新能源汽车的耗铜量是传统能源汽车的两倍，新能源汽车对铜的需求也有拉动作用。如果全球年产500万辆的新能源汽车，将增加铜需求25万吨；同样截止8月份，房地产新开工面积和房地产开发投资完成额同比增长7.6%和7.9%，略有下滑，但是二、三线城市地产去库存时间逐渐缩短，预计未来也不会大幅下滑。▌四、盈利会随着铜价稳企逐渐释放金属的价格周期表现分为两类，一类是倒“Ｖ”走势，也就是“怎么上去，怎么下来”；还有一类则是“上去，横盘，再下跌”，这也是金属市场最喜闻乐见但是不多见的玩法。因为随着横盘，金属价格能够在较高位保持一段时间，这也就有利于企业的滞后的盈利逐渐释放，带动股价暴涨。下面，笔者举两个历史上的栗子。第一个也就是发生在铜行业的栗子，２００２年初到２００６年５月，ＬＭＥ铜一度从２０００美元／吨上涨到８０００美元／吨，而江西铜业的Ｈ股股价也从不到１港元涨到７港元。接下来的２００６年５月到２００７年１０月，ＬＭＥ铜基本是震荡维持在了８０００美元／吨左右的高位，随着盈利的逐步释放，江西铜业的股价也就从７港元飙升到了３２港元，涨幅达到约３５０％。第二个是发生在碳酸锂行业的栗子，２０１４年底碳酸锂价格从４万／吨上涨到２０１６年５月的１９万／吨，这期间天齐锂业（００２４６６.ｓｚ）的股价涨幅达４００％。之后随着碳酸锂价格的高位稳企，盈利逐步释放，２０１７年天齐锂业的股价继续上涨约２００％。目前，铜价已经是处在近三年来的一个高位。前文也已经说到，铜价的后市基本比较明朗，那么铜企在套期保值上大亏损的概率也就降低。再加上供需关系收紧，那么铜价能不能保持在一个较高位的价格，带动铜股的爆发，也就值得期待了。▌五、投资机会江西铜业，是集采矿、选矿、冶炼、加工、贸易、技术为一体的国内最大的综合性铜生产企业之一，旗下子公司众多，贯通铜行业产业链条。控股的主要矿山有德兴铜矿、永平铜矿、城门山铜矿、武山铜矿、东乡铜矿以及银山铅锌矿，并且拥有国内规模最大的铜冶炼厂－贵溪冶炼厂。截止2016年底，江西铜业铜金属资源总量983万吨，权益铜资源储量为443万吨，铜精矿含铜产能２１万吨／年；除此之外，公司的财务状况良好，目前的资产负债率仅为48.20％，远低于国内铜行业其他公司水平。紫金矿业，虽然看名字是金概念股，但是近几年其铜业务的营收已经逐渐取代金矿业务。且公司极具有商业投资嗅觉，在前几年大宗商品还处于低迷的时期，紫金矿业就陆续收购了澳洲亚诺顿金矿、刚果（金）科卢齐铜矿、黑龙江多宝山铜矿等大型项目，并且合作开发巴新波格拉金矿以及刚果（金）卡莫阿铜矿。紫金矿业的铜资源储量也非常高，截止2016年底，公司拥有铜资源储量3006万吨，约为江西铜业储量的2.1倍，占全国总资源的将近三分之一。看估值，江西铜业目前的市净率是32.5倍。而紫金矿业的市净率为20.2，相应高的铜的资源，以及较低的估值，待铜价稳企之时，可能会具有更高的弹性。当然，行业若转好，两个龙头都不会差。▌小结综合来看，铜股股价能不能爆发的关键，也就是这周期铜价的走势，到底能不能具备“上去，横盘，再下来”的特征。经济的稳企，以及供需关系的收紧，接下来关于铜的故事，让我们拭目以待~【作者简介】抹茶拿铁|格隆汇·专栏作者长期关注周期行业的二级狗一枚爱阅读爱思考，耐心等待机会返回搜狐，查看更多责任编辑：',
 'edit': '',
 'link': 'http://www.sohu.com/a/200851917_162818',
 'source': '',
 'time': '2017-10-28 19:23',
 'title': '为什么铜价持续上涨，铜股却表现平庸？',
 'type': 'www.sohu.com'}
2017-10-28 22:15:05,921  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'BODY' state, still expecting more data to get to 'FINISHED' state.>]
<GET http://www.sohu.com/a/200854533_143019>
2017-10-28 22:15:05,922  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.sohu.com/a/200854533_143019> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'BODY' state, still expecting more data to get to 'FINISHED' state.>]
2017-10-28 22:15:30,581  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  User timeout caused connection failure: Getting http://business.sohu.com/20171028/n520452931.shtml took longer than 30.0 seconds..
<GET http://business.sohu.com/20171028/n520452931.shtml>
2017-10-28 22:15:30,582  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://business.sohu.com/20171028/n520452931.shtml> (failed 1 times): User timeout caused connection failure: Getting http://business.sohu.com/20171028/n520452931.shtml took longer than 30.0 seconds..
2017-10-28 22:15:30,610  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200855069_632979> from <GET http://business.sohu.com/20171028/n520452931.shtml>
2017-10-28 22:15:33,261  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200855069_632979> (referer: http://news.baidu.com/finance)
2017-10-28 22:15:33,371  [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.sohu.com/a/200855069_632979> (referer: http://news.baidu.com/finance)
Traceback (most recent call last):
  File "E:\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python_code\Spider3\distributed\yixun\yixun\spiders\baidu_finance.py", line 188, in parse_base
    item['title'] = response.xpath('//div[@class="text-title"]/h1/text()').extract()[0]
IndexError: list index out of range
2017-10-28 22:15:38,723  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  User timeout caused connection failure: Getting http://www.sohu.com/a/200854533_143019 took longer than 30.0 seconds..
<GET http://www.sohu.com/a/200854533_143019>
2017-10-28 22:15:38,724  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.sohu.com/a/200854533_143019> (failed 2 times): User timeout caused connection failure: Getting http://www.sohu.com/a/200854533_143019 took longer than 30.0 seconds..
2017-10-28 22:15:38,766  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200854533_143019> (referer: http://news.baidu.com/finance)
2017-10-28 22:15:38,881  [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.sohu.com/a/200854533_143019>
{'about': 'from baidu',
 'author': '',
 'content': '文|陈达导读：今天想来介绍一位我佩服至极的低调大神——对冲基金经理赛斯·克拉曼（SethKlarman）。他和他的对冲基金Baupost妖娆的业绩神话我就不多废话了，有兴趣的可以参见拙作：《像对冲基金经理那样去思考：事件驱动交易以及扯一点关于价投的淡》。本文仅仅想对他的一些投资思想与哲学做一个简单梳理。当然其思想体系波大精深，我也只是说个部分，如果以后有机会，我们再来狗尾续丑雕。从一个关于投资的小故事说起题在遥远的加利福尼亚共和国，有一个美如画的海边小镇叫蒙特雷（Monterey），其附近海域在一百年前曾是沙丁鱼扎堆的大渔场。于是靠海吃海，小镇的渔业与罐头食品业生意昌隆。然而天有不测风云，出于某种不可解释的神奇力量，二战以后那里的沙丁鱼纷纷神秘消失，但岸边上的食品加工厂都是有很高的沉没成本的呀，机器厂房哪能闲置，含着泪也要把工开；于是沙丁鱼价格被炒，沙丁鱼罐头的价格也随之旱地拔葱。如同古人炒郁金香今人炒大红袍，这沙丁鱼罐头上来就是一波恶炒。某天有一个炒家一时兴起打算对自己好一点，于是晚饭就开了一个巨贵无比的沙丁鱼罐头来吃，以示哥们我就是这么土豪。结果该罐头已变质，哥们差点挂掉。于是他找到罐头的卖家理论，说你这罐头有质量问题。结果卖家斜瞄了他一眼，幽幽地说：你这个瓜娃子，这个是交易罐头，不是用来吃的。所以也有一段时间郁金香不是拿来种的，大红袍不是拿来喝的，他们都是交易花和交易茶，你要是傻兮兮地拿来泡茶，只能落得人丑茶凉的结局。投机炒客对于买卖决定的判断来自于对其他交易对手行为的预测；但根据克大神，一个克己的投资者，他们收益的着眼点应该是：一、从某一门生意中涓涓而来的自由现金流，最终要么反映为更高的股价，要么反映为分红股利；二、从其他投资者愿意支付的溢价，反映为更高的估值乘数；三、从股价与内在价值的鸿沟的不断的坍塌缩短而来。克神谈价投题在为格雷厄姆老爷子的《证券投资》（SecurityAnalysis）第六版所做的序言里，克拉曼谈了谈他眼中的价值投资：啥叫价投，是以资产的价值以下的价格进行投资，留出足够的安全边际。而安全边际的使命，是在你的血汗钱与你的失算、出错、厄运和经济与市场本身的风云诡谲之间搭起缓冲区域。很多人认为价投就是“找个低价来投”，这就有点肤浅，价投是一套哲学小宇宙，包括了深度的基本面分析、追求长期投资目标、风险控制与拒绝从众等等漫天漫宇的小星星。如果认为价投就是“找个低价来投”，那么你有可能，低价买了个破产货出局（不做基本面分析）；低价买了但是小亏止损出局（没有长期持股的目标）；低价融资壕一把结果爆仓出局（无风控意识）；低价买了结果在一片唱衰声中黯然出局（顶不住群众大流的压力与背离人群的恐惧）。克神的另外一个观点就比较极端了，容易让人感觉受到了侮辱：他认为价投所需要的人格特质——无论是沉静、耐性、纪律和反风险——都是基因决定的。所以据此观点，你是不是一个价投在爹妈当年擦出的那一次火花中就已经尘埃落定了，你要么天生就是价投，要么永远都不会是。他认为，你听到价投的理念，要么你深深地发出共鸣，要么觉得这他妈是什么**玩意儿，这就是你基因深处的一次理性与情感的喷薄。类似的观点巴菲特也曾经感慨过。在《格雷厄姆-多德村的超级投资家》（TheSuperinvestorsofGraham-and-Doddsville）中巴老表示：（价值投资）花四毛钱买一块钱资产的这个念头，人们要么是秒懂，要么是永远都不懂；这就跟移花接木一样，要么有些直接就参天大树，剩下的那些人你就算传道一百年、拿着各种证据啊记录啊摊开在他面前，也是鸡同鸭讲，不同世界的人没必要强行沟通。按照这个观点，我们不用担心全市场都会变成夹头从而消弭价值投资的超额收益，或者担心雪球被夹头们沦陷，因为这不可能嘛——一部分人”从基因上不可能”成为夹头。最安全边际题有人问巴菲特，投资里有没有四字真言送给我们这些韭菜丫？这其实是个韭菜问题，但巴菲特还真就想了想，郑重其事地回答说：有，送你四个字：安全边际。他认为这是投资里最重要的四个汉字（或三个单词marginofsafety）。于是我们就听到了铺天盖地对“安全边际”的喧哗与骚动。我知道对于很多人而言这玩意儿都要听得都恶心了，但我也知道对于很多人而言未必知道这个四字真言的确切含义。而克神比巴老对于安全边际要更加执念一点，他不仅嘴上说安全边际重要，还特意写了一本目前市面上价格三千美金的书，来彻底阐述安全边际的重要性，并将该书取名为——请原谅理科生的直男思维——《安全边际》。安全边际一词，第一次出现是在格雷厄姆祖宗的《证券分析》一书里，原文定义是股票现价与它的最低的内在价值之间的折扣。可能有人会说不对啊这里有个逻辑问题，市场为啥要打折卖这不是缺心眼吗？这就回到了格雷厄姆关于Mr.Market的那个隐喻，Mr.Market是典型的双向情感障碍（bipolar）患者，也就是同时罹患狂躁症与抑郁症的病人。在狂躁的时候他兴高采烈地溢价买卖，在抑郁的时候他心灰意冷地折价买卖，所以就会有打折的可能，就有出现安全边际的可能。说起来也许略抽象，画个图来巩固一下。如果你相信市场是完全有效的而不是一个双向情感障碍患者，那么安全边际也不要去想了，因为对你而言它并不存在。从图中来看这个安全边际的概念看似简单朴素得喜闻乐见，其实略微还是有点深意的。第一，内在价值是“最低的内在价值”（minimumintrinsicvalue），有人可能会问奇怪了内在价值为啥还有最高最低，它不就是一数字么？其实不然，因为你去问任何夹头大神，无论是巴菲特还是克拉曼甚至是格雷厄姆，你去问他们000932的内在价值是多少，他们一定不会告诉你一个精确数字，这个只有神棍才能做到。估值精确到一个点，统计上这叫点估计，统计学告诉我们点估计都是不靠谱的，所以我们的内在价值一定是一个大概的模糊的区间。做过比较深度的估值练*的同学们都知道最后出来的结果一般而言就是一个区间——比如我算出来000932的内在价值大概是在1块钱到20块钱之间——因为我估值不可能只用一种方法来估，如果你通过不同的估值方法却纷纷做出来同样一个结果，那就是无耻的巧合，我们会怀疑你是不是在搞反向工程。想要不无耻你就只能给区间，因为那么多个结果谁对谁错你其实并无把握；不耍流氓的天气预报一定给的是降雨概率，而不耍流氓的估值分析一定给的是价格区间。古谚有云，含糊的正确要好过精准的错误。我们对内在价值的估计是含糊的，所以就存在最低的内在价值，而安全边际——至少在格老爷子的心中——是以最低的内在价值为衣钵的。在最低的内在价值上打出的折扣才是安全边际。格老爷子的意思是，我认为000932的内在价值是5块，但我徒弟巴菲特认为值4块，而他的好基友芒格认为只值3块，好吧爱徒及其基友的话我是要听的，所以我综合认为000932最低的内在价值就是3快；而现在000932正好卖3块，我买不买呢？当然不买，值3块的东西我花3块买岂不是在蹉跎青春，那必须再打个六折我才会去考虑——其中的40%off就是安全边际，六折以下的价格就是买点。而巴菲特对此的理解是另一个鲜活的隐喻：我建一座能承重三万磅的桥，但却只让通一万磅的车；这样就算我大意了失算了漏放了一辆一万两千磅或者一万三千磅的卡车过去，也不至于桥毁人亡。所以上面的那一个插图，就可以进一步演化为：这是价值投资中安全边际的完整含义。很多人说价投是不管成本的，要与持股成本相忘于江湖；但这在格雷厄姆老爷子那里简直是欺师灭祖，因为格老爷子说：安全边际靠啥？就是靠你支付的成本。对于任何证券而言，在某一个价格上也许有安全边际，但在另一个更高的价格上安全边际可能就烟消云散了。所以这事全看你买入的价格，成本就是你安全边际的命根儿。霍华德·马克斯（HowardMarks）也说过类似的话，“低价是防止出错的边际的最终来源”（“Lowpriceistheultimatesourceofmarginforerror”）。那什么样的资产可以提供安全边际呢？克拉曼认为首先必须是有形资产，他认为无形资产飘忽不定，弹性太大，如果出了啥幺蛾子，可能最后就屁都不值了。所以不能指望无形资产来提供安全边际，丫自己就不太安全。而反观有形资产则具有“替代用途”，比如你用一家店面来开个面店，结果跪了，你还可以转给别人开烧烤店；但是你原来“王大财拉面店”的品牌价值，如果找不到欣赏你的接盘侠，可能就一命呜呼了。于是谈安全边际，克神只看有形资产。所以克神在安全边际理论上又加了一些条件，这也是我目前所知道的对于此理论最为保守最为苛刻的解读，简直可以称其为“最安全边际”。他说啥叫安全边际啊，就是以该生意最低内在价值的巨大折扣价去投资该生意，并且偏爱于有形资产；然后我们不指望这笔生意经营得有多风生水起，哪怕它此时此刻就在我眼前被清算了，我也能够从中赚到钱。读到这里你肯定会微微一笑：别逗了怎么可能有这种好事？真有，只要你会找，只要你能等。双向情感障碍的Mr.市场会有极度抑郁的时期，尤其是对待一些人神共愤的扑街公司，简直有不鞭尸就浑身难受的变态情结。克神举例说在八十年代有一家叫ErieLackawanna的公司，而该公司当时创建的目的是为了清算掉在1976年就已经停止营运的ErieLackawanna铁路公司。至1987年年末，当时ErieLackawanna的股价大概交易在110美元的位置，但是公司账面上却有每股140美元的现金，另外还有向美国国税局索要巨额退税的请求权。即使不算退税，140美元的现金也已经超过了当时的股价，投资该公司的向下风险几乎为零，无非最多就是短期内会有浮亏。而到了1991年中旬，该公司累计向股东清算派发了179美元的现金，派发后剩余的资产仍以每股8美元在交易。当然这种投资机会当然如同凤毛麟角，所以克神常常持有大量现金，随随便便都是三到五成现金，没机会就不出手。这真正是做到知行合一的一个好榜样，除了佩服，我没有其他感受。一定要警惕伪价投题咱事先说明啊不做任何道德判断，伪价投不是伪君子。伪价投不一定是故意伪的，有可能你也真心想价投，但是理解有偏差或者能力跟不上，结果没投对；也有可能号称自己是价投，但是打心眼里其实根本不相信那一套，并不行价投之事。但是这些都没啥，无关乎道德，最多就亏点自己的钱呗；最可耻的是那些以价投作为幌子来吸引资产，而其投资哲学事实上与价投没有半毛钱关系的基金经理，克神给他们贴了一个标签——投资变色龙；骂完了他想想觉得意犹未尽，于是又多贴了一张标签——江湖老神棍。伪价投的神棍们利用八十年代价投大师们的声名鹊起——诸如巴菲特、MichaelPrice等人逐渐家喻户晓——而变色为价值投资者来招商引资。他们并不是骨子里的价投践行者，有些人可能自己也没弄明白到底啥是价投；还有一些更可恨，不管自己懂不懂，反正他们知道客户们大概率都不懂，于是他们利用大众对价投的片面理解来饱自己的钱囊。明面上为价投，实则是收人头。比如他们可能追涨某个市盈率已到五位数的IT股票，他说我这就是价投，你看未来二十一世纪就是电脑的天下，IT业多么有价值啊，你听我的，投这个股票就是投资价值。再比如有些哥们专买下跌股，格雷厄姆不是说了嘛安全边际要靠成本来实现，追涨是韭菜，那追跌我总是价投了吧？但是买下跌股与买便宜股之间还差着一个叶荣添与巴菲特的距离（Tobuythedipisnottobuythecheap）。专拣一泻千里的困境股越跌越买，这不是价投，这是断头。还有一些哥们那就是卷起袖子直接抄作业了啊，巴菲特买什么我就买什么，这总算是价投了吧？我就呵呵了，你这最多算资产托管啊北鼻。这些哥们显然都是伪到不行的价投，你可能会问，达某你在这里说几句风凉话难道就能独善其身了吗？我今天必须向大家来忏悔，我常说自己“大概持价投理念”（我不太敢直接以价投自居）；但是我要坦白，我可能就是一个伪价投。当夜深人静之时我自问每次交易都谨遵安全边际了吗，答案当然是一个大写的不，这尼玛我要是都能做到岂不是成了自虐狂的修道士了？没错，真正的价投就是对自己无比严苛的修道士，而我自问还远远还做不到——大概是因为尘世的喧嚣与内心的浮躁，或者大概是因为我没有那个基因。所以我不敢说自己是一个价投。但是我并不惭愧啊，这个世界上说自己是价投的百分之九十九都是伪价投；你如果认为自己是一个价投，那百分之九十九你也是伪价投。这话听着虽然刺耳，但是百分之九十九是正确的判断。也许看到这里你马上就不服了，我们不妨来做一个思维实验吧。纯粹假设啊：你的某个高高在上的女神股——咱比如说茅台吧——出于某种非黑天鹅的原因，一个礼拜连续五个跌停板，暴跌了40%，你买不？你会说卧槽这不废话吗我当然买啦。于是你是伪价投，因为咱说好的“内在价值”去哪了？你说不啊我搞过估值了茅台每股内在价值220块啊，现在跌到210块了，我当然买啦。于是你还是伪价投，因为咱说好的“最低的内在价值”去哪了？你说不啊我说错了我搞过估值了茅台的内在价值是220块-250块之间，所以我买啦。于是你仍然是伪价投，因为咱说好的“安全边际”去哪了？你估值茅台220-250，你给自己一个40%的安全边际，于是乎茅台跌到132块你才考虑去买，这才是真价投。你说这怎么可能呐，茅台跌到132块你是在做梦吗？道理就在这里，如果跌不到132块你就别去投资茅台呗，世界上那么多的股票，哪个能跌到，你再去买哪个。如果这个世界一片歌舞升平实在是找不到这样的投资标的，那你就拿着现金呗，这才是真正的价投，而最难能可贵的其实就是在这种时候能够“忍得住”。而这样的真价投的基金经理，恕鄙人见识浅陋，目前只见过赛斯·克拉曼一个；就连巴菲特大神，我都没有十足把握说他是一个这样的教科书一般的价值投资者。我自问反正我自己是肯定做不到如此去投资的，所以我不敢说自己是价投，我只能是“大概上持价投理念”；而咱也不要总是把“安全边际”当做一个投资术语放在嘴上把玩，真正能做到安全边际的，自古而今，又有几人？P.S.到二月我刚好来雪球一整年了，感谢大家也感谢雪球一年以来对我的支持与错爱。如无意外，只要大家愿意看，我就愿意继续写，为了我们大家热爱的投资事业。利益披露：作者在文章发表时不持有文中提到任何股票的仓位。本文行文仓莽，如有不足之处，还请各位海涵斧正。转载自：雪球，作者：陈达美股投资返回搜狐，查看更多责任编辑：',
 'edit': '',
 'link': 'http://www.sohu.com/a/200854533_143019',
 'source': '',
 'time': '2017-10-28 19:04',
 'title': '从Seth Klarman说起，告诉你真正的安全边际',
 'type': 'www.sohu.com'}
2017-10-28 22:15:38,885  [scrapy.core.engine] INFO: Closing spider (finished)
2017-10-28 22:15:38,886  [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 11809,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 446849,
 'downloader/response_count': 26,
 'downloader/response_status_count/200': 22,
 'downloader/response_status_count/301': 3,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 10, 28, 14, 15, 38, 885477),
 'item_scraped_count': 2,
 'log_count/DEBUG': 32,
 'log_count/ERROR': 8,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 29,
 'scheduler/enqueued/memory': 29,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2017, 10, 28, 14, 14, 54, 888961)}
2017-10-28 22:15:38,886  [scrapy.core.engine] INFO: Spider closed (finished)
2017-10-28 22:17:36,232  [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: yixun)
2017-10-28 22:17:36,233  [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'yixun', 'CONCURRENT_REQUESTS': 1000, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_TIMEOUT': 50, 'LOG_FILE': 'log/20171028/22_baidu_finance_worker.log', 'LOG_FORMAT': '%(asctime)s,%(msecs)d  [%(name)s] %(levelname)s: %(message)s', 'NEWSPIDER_MODULE': 'yixun.spiders', 'RETRY_HTTP_CODES': [500, 502, 503, 504, 400, 408, 404, 403], 'RETRY_TIMES': 5, 'SPIDER_MODULES': ['yixun.spiders']}
2017-10-28 22:17:36,279  [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-10-28 22:17:36,790  [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'yixun.middlewares.ChoiceAgent',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'yixun.middlewares.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-10-28 22:17:36,793  [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'yixun.middlewares.ProcessResponseMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-10-28 22:17:36,794  [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-10-28 22:17:36,794  [scrapy.core.engine] INFO: Spider opened
2017-10-28 22:17:36,803  [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-10-28 22:17:36,803  [baidu_finance] INFO: Spider opened: baidu_finance
2017-10-28 22:17:36,805  [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024
2017-10-28 22:17:36,903  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.baidu.com/finance> (referer: None)
2017-10-28 22:17:37,101  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://business.sohu.com/20171028/n520449743.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,119  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dzwww.com/xinwen/shehuixinwen/201710/t20171028_16585105.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,138  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.ifeng.com/a/20171028/52834010_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,170  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.qq.com/a/20171028/027677.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,185  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362753.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,193  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://industry.people.com.cn/n1/2017/1028/c413883-29614276.html> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,201  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.cecb2b.com/info/20171028/3613657.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,210  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biz.jrj.com.cn/2017/10/28191423299779.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,222  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.chinabyte.com/265/14329265.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,234  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28212023299870.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,263  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.ifeng.com/a/20171028/15751804_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,272  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/cj/2017/10-28/8362769.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:37,312  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]
<GET http://www.sohu.com/a/200850605_313745>
2017-10-28 22:17:37,312  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.sohu.com/a/200850605_313745> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]
2017-10-28 22:17:37,335  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 69924: illegal multibyte sequence>
2017-10-28 22:17:37,814  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350864> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:39,141  [baidu_finance] ERROR: <twisted.python.failure.Failure zlib.error: Error -3 while decompressing data: invalid distance too far back>
2017-10-28 22:17:39,267  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28205123299849.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:39,368  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72411: illegal multibyte sequence>
2017-10-28 22:17:39,788  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200854533_143019> from <GET http://business.sohu.com/20171028/n520452220.shtml>
2017-10-28 22:17:39,964  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/ty/shipin/cns-d/2017/10-28/news738503.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:40,93  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362715.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:40,264  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350881> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:42,216  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28211923299869.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:42,318  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70115: illegal multibyte sequence>
2017-10-28 22:17:45,254  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200854533_143019> (referer: http://news.baidu.com/finance)
2017-10-28 22:17:45,370  [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.sohu.com/a/200854533_143019> (referer: http://news.baidu.com/finance)
Traceback (most recent call last):
  File "E:\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python_code\Spider3\distributed\yixun\yixun\spiders\baidu_finance.py", line 188, in parse_base
    item['title'] = response.xpath('//div[@class="text-title"]/h1/text()').extract()[0]
IndexError: list index out of range
2017-10-28 22:18:12,542  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  User timeout caused connection failure: Getting http://business.sohu.com/20171028/n520452931.shtml took longer than 30.0 seconds..
<GET http://business.sohu.com/20171028/n520452931.shtml>
2017-10-28 22:18:12,543  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://business.sohu.com/20171028/n520452931.shtml> (failed 1 times): User timeout caused connection failure: Getting http://business.sohu.com/20171028/n520452931.shtml took longer than 30.0 seconds..
2017-10-28 22:18:12,567  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200855069_632979> from <GET http://business.sohu.com/20171028/n520452931.shtml>
2017-10-28 22:18:13,868  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200855069_632979> (referer: http://news.baidu.com/finance)
2017-10-28 22:18:13,976  [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.sohu.com/a/200855069_632979>
{'about': 'from baidu',
 'author': '',
 'content': '想象一下，如果把人的细胞、血液、肿瘤组织储存到一家“银行”，当科研或者临床研究需要时，再把它们提取出来，集中转化，这将对医药研发、健康产业、科研成果转化等领域起到多么巨大的推动作用？如今，这样的“银行”已经在上海开张了。▲走进生物银行，高高的橙色架子上有一个个类似保险柜的小抽屉，这里储存了50万份肿瘤组织，是银行的第一批“库存”。近日，投资近亿元的国内规模最大的“生物银行”在上海张江正式启用。与一般银行不同的是，生物银行的储存品不是钱，而是组织、血液、细胞、器官、DNA等生物样本以及与其相关的临床、病理、治疗、随访、知情同意等资料，以及质量控制、信息管理与应用系统。▲抽屉里面放着邮票大小的肿瘤组织切片，通过石蜡技术常温保存，每个抽屉可以储存126份样本。上海张江生物银行于2016年正式批复成立，由张江高科技园区管理委员会立项，生物芯片上海国家工程研究中心牵头承建，是张江科学城优先启动的重要基础设施工程项目之一。▲为了保护患者隐私，组织框上只有编号和二维码，患者的诊断情况、家族病史、生活习性等信息都被记录在数据库中。▲通过合法科学的信息技术手段，科研人员可快速便捷地从海量的数据库中调取相关数据和信息。生物银行为张江、上海、全国乃至全球重大疾病基础与临床研究提供了重要的生物样本资源；成为药物研发、临床诊治技术研发、健康研究与产业化的最重要环节、最宝贵资源；是功能基因组研究众多重要基因、蛋白等科研成果快速产业化、应用到临床的重要保证。▲生物银行还引进了我国第一台商业化全自动存储冰箱。▲冰箱的主存储温度为-82摄氏度，多用于储存血清、血浆、血细胞和DNA核酸等样品，缓冲间温度则稍高一些，为-20摄氏度。根据国家战略部署，张江科学城正积极打造世界一流的医学科技创新中心，在信息技术、生命科学等领域先行布局了一批科技创新服务平台。▲机械臂会精准取样，并自动将样本传送到科研人员手中。▲气相液氮罐存放的主要是各类细胞和一些需要新鲜研究的肿瘤组织，里面温度可以达到-196摄氏度。上海张江生物银行作为其中的研发与转化功能型平台项目，在张江研发服务平台功能提升方面发挥了积极的作用，不但开创了我国集约化生物样本资源库新模式，制定了重大疾病生物样本资源国家系列标准，还实现了科研成果快速产业化，惠及普通老百姓。▲科研人员可以对组织进行穿心取样，将微量组织提取出来，和几十甚至上百个这样的微量组织做成组织芯片，可供科研人员做上百次不同的实验。▲制作完成的样本被存放入生物银行。除了科研机构外，普通人也可享受生物银行的服务，平均每个样本每年的存储费只有6元钱左右。据介绍，生物银行一期已达500万样本存储能力，二期达到1000万样本储存能力，三期工程启动在建，预计总投资3到5亿元，储存量达到5000万。记者柏可林返回搜狐，查看更多责任编辑：',
 'edit': '',
 'link': 'http://www.sohu.com/a/200855069_632979',
 'source': '',
 'time': '2017-10-28 19:10',
 'title': '视线 | 把细胞存入“银行”：探访国内最大的“生物银行”',
 'type': 'www.sohu.com'}
2017-10-28 22:18:13,979  [scrapy.core.engine] INFO: Closing spider (finished)
2017-10-28 22:18:13,980  [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 10662,
 'downloader/request_count': 26,
 'downloader/request_method_count/GET': 26,
 'downloader/response_bytes': 440500,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 22,
 'downloader/response_status_count/301': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 10, 28, 14, 18, 13, 979348),
 'item_scraped_count': 1,
 'log_count/DEBUG': 27,
 'log_count/ERROR': 7,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 26,
 'scheduler/dequeued/memory': 26,
 'scheduler/enqueued': 26,
 'scheduler/enqueued/memory': 26,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2017, 10, 28, 14, 17, 36, 803222)}
2017-10-28 22:18:13,980  [scrapy.core.engine] INFO: Spider closed (finished)
2017-10-28 22:34:57,48  [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: yixun)
2017-10-28 22:34:57,48  [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'yixun', 'CONCURRENT_REQUESTS': 1000, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_TIMEOUT': 50, 'LOG_FILE': 'log/20171028/22_baidu_finance_worker.log', 'LOG_FORMAT': '%(asctime)s,%(msecs)d  [%(name)s] %(levelname)s: %(message)s', 'NEWSPIDER_MODULE': 'yixun.spiders', 'RETRY_HTTP_CODES': [500, 502, 503, 504, 400, 408, 404, 403], 'RETRY_TIMES': 5, 'SPIDER_MODULES': ['yixun.spiders']}
2017-10-28 22:34:57,93  [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-10-28 22:34:57,578  [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'yixun.middlewares.ChoiceAgent',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'yixun.middlewares.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-10-28 22:34:57,581  [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'yixun.middlewares.ProcessResponseMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-10-28 22:34:57,582  [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-10-28 22:34:57,582  [scrapy.core.engine] INFO: Spider opened
2017-10-28 22:34:57,593  [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-10-28 22:34:57,593  [baidu_finance] INFO: Spider opened: baidu_finance
2017-10-28 22:34:57,595  [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024
2017-10-28 22:34:57,706  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.baidu.com/finance> (referer: None)
2017-10-28 22:34:57,901  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200850605_313745> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:57,919  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200851917_162818> from <GET http://business.sohu.com/20171028/n520449743.shtml>
2017-10-28 22:34:57,953  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dzwww.com/xinwen/shehuixinwen/201710/t20171028_16585105.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:57,997  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.ifeng.com/a/20171028/52834010_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,3  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://industry.people.com.cn/n1/2017/1028/c413883-29614276.html> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,12  [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.sohu.com/a/200850605_313745> (referer: http://news.baidu.com/finance)
Traceback (most recent call last):
  File "E:\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python_code\Spider3\distributed\yixun\yixun\spiders\baidu_finance.py", line 188, in parse_base
    item['title'] = response.xpath('//div[@class="text-title"]/h1/text()').extract()[0]
IndexError: list index out of range
2017-10-28 22:34:58,32  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biz.jrj.com.cn/2017/10/28191423299779.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,36  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.qq.com/a/20171028/027677.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,40  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28211923299869.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,52  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.cecb2b.com/info/20171028/3613657.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,57  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.ifeng.com/a/20171028/15751804_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,67  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.chinabyte.com/265/14329265.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,128  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362753.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,141  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 69925: illegal multibyte sequence>
2017-10-28 22:34:58,141  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 69925: illegal multibyte sequence>
2017-10-28 22:34:58,182  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/cj/2017/10-28/8362769.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:58,233  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350864> (referer: http://news.baidu.com/finance)
2017-10-28 22:34:59,831  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200854533_143019> from <GET http://business.sohu.com/20171028/n520452220.shtml>
2017-10-28 22:35:00,165  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350881> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:00,581  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/ty/shipin/cns-d/2017/10-28/news738503.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:00,663  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362715.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:00,927  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28205123299849.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:01,28  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72221: illegal multibyte sequence>
2017-10-28 22:35:01,28  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72221: illegal multibyte sequence>
2017-10-28 22:35:01,856  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200855069_632979> from <GET http://business.sohu.com/20171028/n520452931.shtml>
2017-10-28 22:35:02,872  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28212023299870.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:02,973  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 69734: illegal multibyte sequence>
2017-10-28 22:35:02,974  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 69734: illegal multibyte sequence>
2017-10-28 22:35:04,830  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200851917_162818> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:04,931  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xff in position 10649: illegal multibyte sequence>
2017-10-28 22:35:04,932  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xff in position 10649: illegal multibyte sequence>
2017-10-28 22:35:06,840  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200854533_143019> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:06,958  [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.sohu.com/a/200854533_143019> (referer: http://news.baidu.com/finance)
Traceback (most recent call last):
  File "E:\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python_code\Spider3\distributed\yixun\yixun\spiders\baidu_finance.py", line 188, in parse_base
    item['title'] = response.xpath('//div[@class="text-title"]/h1/text()').extract()[0]
IndexError: list index out of range
2017-10-28 22:35:09,792  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200855069_632979> (referer: http://news.baidu.com/finance)
2017-10-28 22:35:09,907  [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.sohu.com/a/200855069_632979> (referer: http://news.baidu.com/finance)
Traceback (most recent call last):
  File "E:\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python_code\Spider3\distributed\yixun\yixun\spiders\baidu_finance.py", line 188, in parse_base
    item['title'] = response.xpath('//div[@class="text-title"]/h1/text()').extract()[0]
IndexError: list index out of range
2017-10-28 22:35:09,912  [scrapy.core.engine] INFO: Closing spider (finished)
2017-10-28 22:35:09,914  [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 10102,
 'downloader/request_count': 25,
 'downloader/request_method_count/GET': 25,
 'downloader/response_bytes': 449571,
 'downloader/response_count': 25,
 'downloader/response_status_count/200': 22,
 'downloader/response_status_count/301': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 10, 28, 14, 35, 9, 913456),
 'log_count/DEBUG': 26,
 'log_count/ERROR': 11,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 25,
 'scheduler/dequeued/memory': 25,
 'scheduler/enqueued': 25,
 'scheduler/enqueued/memory': 25,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2017, 10, 28, 14, 34, 57, 592751)}
2017-10-28 22:35:09,914  [scrapy.core.engine] INFO: Spider closed (finished)
2017-10-28 22:41:26,288  [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: yixun)
2017-10-28 22:41:26,289  [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'yixun', 'CONCURRENT_REQUESTS': 1000, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_TIMEOUT': 50, 'LOG_FILE': 'log/20171028/22_baidu_finance_worker.log', 'LOG_FORMAT': '%(asctime)s,%(msecs)d  [%(name)s] %(levelname)s: %(message)s', 'NEWSPIDER_MODULE': 'yixun.spiders', 'RETRY_HTTP_CODES': [500, 502, 503, 504, 400, 408, 404, 403], 'RETRY_TIMES': 5, 'SPIDER_MODULES': ['yixun.spiders']}
2017-10-28 22:41:26,340  [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-10-28 22:41:26,820  [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'yixun.middlewares.ChoiceAgent',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'yixun.middlewares.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-10-28 22:41:26,823  [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'yixun.middlewares.ProcessResponseMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-10-28 22:41:26,824  [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-10-28 22:41:26,824  [scrapy.core.engine] INFO: Spider opened
2017-10-28 22:41:26,833  [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-10-28 22:41:26,834  [baidu_finance] INFO: Spider opened: baidu_finance
2017-10-28 22:41:26,835  [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024
2017-10-28 22:41:26,974  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.baidu.com/finance> (referer: None)
2017-10-28 22:41:27,143  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://business.sohu.com/20171028/n520469270.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,212  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.stockstar.com/SS2017102800002270.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,264  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://industry.people.com.cn/n1/2017/1028/c413883-29614276.html> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,274  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/cj/2017/10-28/8362769.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,282  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.cecb2b.com/info/20171028/3613657.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,308  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.ifeng.com/a/20171028/15751804_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,321  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.chinabyte.com/265/14329265.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,328  [scrapy.core.scraper] DEBUG: Scraped from <200 http://finance.stockstar.com/SS2017102800002270.shtml>
{'about': 'from baidu',
 'author': '',
 'content': '（原标题：江苏：到2020年粮食生产基本实现全程机械化）摘要【江苏：到2020年粮食生产基本实现全程机械化】江苏省财政今年新增投入1.1亿元专项用于省级粮食生产全程机械化示范县建设，地方财政累计配套投入7.9亿元，其中34个示范县地方财政投入配套资金4.74亿元。江苏粮食生产全程机械化六大环节中，机耕、机收和秸秆机械化还田水平继续保持高位，薄弱环节机械化水平实现稳定提升，预计到2020年，粮食生产基本实现全程机械化。(新华社)江苏省财政今年新增投入1.1亿元专项用于省级粮食生产全程机械化示范县建设，地方财政累计配套投入7.9亿元，其中34个示范县地方财政投入配套资金4.74亿元。江苏粮食生产全程机械化六大环节中，机耕、机收和秸秆机械化还田水平继续保持高位，薄弱环节机械化水平实现稳定提升，预计到2020年，粮食生产基本实现全程机械化。',
 'edit': '',
 'link': 'http://finance.stockstar.com/SS2017102800002270.shtml',
 'source': '新华社',
 'time': '2017-10-28 20:13:13',
 'title': '江苏：到2020年粮食生产基本实现全程机械化',
 'type': 'finance.stockstar.com'}
2017-10-28 22:41:27,334  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biz.jrj.com.cn/2017/10/28191423299779.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,378  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362753.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,395  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.ifeng.com/a/20171028/52834010_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,447  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28205123299849.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,492  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350864> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:27,548  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72887: illegal multibyte sequence>
2017-10-28 22:41:27,548  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72887: illegal multibyte sequence>
2017-10-28 22:41:28,857  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/ty/shipin/cns-d/2017/10-28/news738503.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:28,869  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28211923299869.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:28,970  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70591: illegal multibyte sequence>
2017-10-28 22:41:28,970  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70591: illegal multibyte sequence>
2017-10-28 22:41:29,230  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350881> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:29,549  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362715.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:29,565  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200851917_162818> from <GET http://business.sohu.com/20171028/n520449743.shtml>
2017-10-28 22:41:31,790  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200854533_143019> from <GET http://business.sohu.com/20171028/n520452220.shtml>
2017-10-28 22:41:31,806  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28212023299870.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:31,907  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70400: illegal multibyte sequence>
2017-10-28 22:41:31,907  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70400: illegal multibyte sequence>
2017-10-28 22:41:33,936  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200855069_632979> from <GET http://business.sohu.com/20171028/n520452931.shtml>
2017-10-28 22:41:35,934  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200851917_162818> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:36,45  [scrapy.core.scraper] ERROR: Spider error processing <GET http://www.sohu.com/a/200851917_162818> (referer: http://news.baidu.com/finance)
Traceback (most recent call last):
  File "E:\Anaconda3\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\python_code\Spider3\distributed\yixun\yixun\spiders\baidu_finance.py", line 188, in parse_base
    item['title'] = response.xpath('//div[@class="text-title"]/h1/text()').extract()[0]
IndexError: list index out of range
2017-10-28 22:41:38,115  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200854533_143019> (referer: http://news.baidu.com/finance)
2017-10-28 22:41:38,250  [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.sohu.com/a/200854533_143019>
{'about': 'from baidu',
 'author': '',
 'content': '文|陈达导读：今天想来介绍一位我佩服至极的低调大神——对冲基金经理赛斯·克拉曼（SethKlarman）。他和他的对冲基金Baupost妖娆的业绩神话我就不多废话了，有兴趣的可以参见拙作：《像对冲基金经理那样去思考：事件驱动交易以及扯一点关于价投的淡》。本文仅仅想对他的一些投资思想与哲学做一个简单梳理。当然其思想体系波大精深，我也只是说个部分，如果以后有机会，我们再来狗尾续丑雕。从一个关于投资的小故事说起题在遥远的加利福尼亚共和国，有一个美如画的海边小镇叫蒙特雷（Monterey），其附近海域在一百年前曾是沙丁鱼扎堆的大渔场。于是靠海吃海，小镇的渔业与罐头食品业生意昌隆。然而天有不测风云，出于某种不可解释的神奇力量，二战以后那里的沙丁鱼纷纷神秘消失，但岸边上的食品加工厂都是有很高的沉没成本的呀，机器厂房哪能闲置，含着泪也要把工开；于是沙丁鱼价格被炒，沙丁鱼罐头的价格也随之旱地拔葱。如同古人炒郁金香今人炒大红袍，这沙丁鱼罐头上来就是一波恶炒。某天有一个炒家一时兴起打算对自己好一点，于是晚饭就开了一个巨贵无比的沙丁鱼罐头来吃，以示哥们我就是这么土豪。结果该罐头已变质，哥们差点挂掉。于是他找到罐头的卖家理论，说你这罐头有质量问题。结果卖家斜瞄了他一眼，幽幽地说：你这个瓜娃子，这个是交易罐头，不是用来吃的。所以也有一段时间郁金香不是拿来种的，大红袍不是拿来喝的，他们都是交易花和交易茶，你要是傻兮兮地拿来泡茶，只能落得人丑茶凉的结局。投机炒客对于买卖决定的判断来自于对其他交易对手行为的预测；但根据克大神，一个克己的投资者，他们收益的着眼点应该是：一、从某一门生意中涓涓而来的自由现金流，最终要么反映为更高的股价，要么反映为分红股利；二、从其他投资者愿意支付的溢价，反映为更高的估值乘数；三、从股价与内在价值的鸿沟的不断的坍塌缩短而来。克神谈价投题在为格雷厄姆老爷子的《证券投资》（SecurityAnalysis）第六版所做的序言里，克拉曼谈了谈他眼中的价值投资：啥叫价投，是以资产的价值以下的价格进行投资，留出足够的安全边际。而安全边际的使命，是在你的血汗钱与你的失算、出错、厄运和经济与市场本身的风云诡谲之间搭起缓冲区域。很多人认为价投就是“找个低价来投”，这就有点肤浅，价投是一套哲学小宇宙，包括了深度的基本面分析、追求长期投资目标、风险控制与拒绝从众等等漫天漫宇的小星星。如果认为价投就是“找个低价来投”，那么你有可能，低价买了个破产货出局（不做基本面分析）；低价买了但是小亏止损出局（没有长期持股的目标）；低价融资壕一把结果爆仓出局（无风控意识）；低价买了结果在一片唱衰声中黯然出局（顶不住群众大流的压力与背离人群的恐惧）。克神的另外一个观点就比较极端了，容易让人感觉受到了侮辱：他认为价投所需要的人格特质——无论是沉静、耐性、纪律和反风险——都是基因决定的。所以据此观点，你是不是一个价投在爹妈当年擦出的那一次火花中就已经尘埃落定了，你要么天生就是价投，要么永远都不会是。他认为，你听到价投的理念，要么你深深地发出共鸣，要么觉得这他妈是什么**玩意儿，这就是你基因深处的一次理性与情感的喷薄。类似的观点巴菲特也曾经感慨过。在《格雷厄姆-多德村的超级投资家》（TheSuperinvestorsofGraham-and-Doddsville）中巴老表示：（价值投资）花四毛钱买一块钱资产的这个念头，人们要么是秒懂，要么是永远都不懂；这就跟移花接木一样，要么有些直接就参天大树，剩下的那些人你就算传道一百年、拿着各种证据啊记录啊摊开在他面前，也是鸡同鸭讲，不同世界的人没必要强行沟通。按照这个观点，我们不用担心全市场都会变成夹头从而消弭价值投资的超额收益，或者担心雪球被夹头们沦陷，因为这不可能嘛——一部分人”从基因上不可能”成为夹头。最安全边际题有人问巴菲特，投资里有没有四字真言送给我们这些韭菜丫？这其实是个韭菜问题，但巴菲特还真就想了想，郑重其事地回答说：有，送你四个字：安全边际。他认为这是投资里最重要的四个汉字（或三个单词marginofsafety）。于是我们就听到了铺天盖地对“安全边际”的喧哗与骚动。我知道对于很多人而言这玩意儿都要听得都恶心了，但我也知道对于很多人而言未必知道这个四字真言的确切含义。而克神比巴老对于安全边际要更加执念一点，他不仅嘴上说安全边际重要，还特意写了一本目前市面上价格三千美金的书，来彻底阐述安全边际的重要性，并将该书取名为——请原谅理科生的直男思维——《安全边际》。安全边际一词，第一次出现是在格雷厄姆祖宗的《证券分析》一书里，原文定义是股票现价与它的最低的内在价值之间的折扣。可能有人会说不对啊这里有个逻辑问题，市场为啥要打折卖这不是缺心眼吗？这就回到了格雷厄姆关于Mr.Market的那个隐喻，Mr.Market是典型的双向情感障碍（bipolar）患者，也就是同时罹患狂躁症与抑郁症的病人。在狂躁的时候他兴高采烈地溢价买卖，在抑郁的时候他心灰意冷地折价买卖，所以就会有打折的可能，就有出现安全边际的可能。说起来也许略抽象，画个图来巩固一下。如果你相信市场是完全有效的而不是一个双向情感障碍患者，那么安全边际也不要去想了，因为对你而言它并不存在。从图中来看这个安全边际的概念看似简单朴素得喜闻乐见，其实略微还是有点深意的。第一，内在价值是“最低的内在价值”（minimumintrinsicvalue），有人可能会问奇怪了内在价值为啥还有最高最低，它不就是一数字么？其实不然，因为你去问任何夹头大神，无论是巴菲特还是克拉曼甚至是格雷厄姆，你去问他们000932的内在价值是多少，他们一定不会告诉你一个精确数字，这个只有神棍才能做到。估值精确到一个点，统计上这叫点估计，统计学告诉我们点估计都是不靠谱的，所以我们的内在价值一定是一个大概的模糊的区间。做过比较深度的估值练*的同学们都知道最后出来的结果一般而言就是一个区间——比如我算出来000932的内在价值大概是在1块钱到20块钱之间——因为我估值不可能只用一种方法来估，如果你通过不同的估值方法却纷纷做出来同样一个结果，那就是无耻的巧合，我们会怀疑你是不是在搞反向工程。想要不无耻你就只能给区间，因为那么多个结果谁对谁错你其实并无把握；不耍流氓的天气预报一定给的是降雨概率，而不耍流氓的估值分析一定给的是价格区间。古谚有云，含糊的正确要好过精准的错误。我们对内在价值的估计是含糊的，所以就存在最低的内在价值，而安全边际——至少在格老爷子的心中——是以最低的内在价值为衣钵的。在最低的内在价值上打出的折扣才是安全边际。格老爷子的意思是，我认为000932的内在价值是5块，但我徒弟巴菲特认为值4块，而他的好基友芒格认为只值3块，好吧爱徒及其基友的话我是要听的，所以我综合认为000932最低的内在价值就是3快；而现在000932正好卖3块，我买不买呢？当然不买，值3块的东西我花3块买岂不是在蹉跎青春，那必须再打个六折我才会去考虑——其中的40%off就是安全边际，六折以下的价格就是买点。而巴菲特对此的理解是另一个鲜活的隐喻：我建一座能承重三万磅的桥，但却只让通一万磅的车；这样就算我大意了失算了漏放了一辆一万两千磅或者一万三千磅的卡车过去，也不至于桥毁人亡。所以上面的那一个插图，就可以进一步演化为：这是价值投资中安全边际的完整含义。很多人说价投是不管成本的，要与持股成本相忘于江湖；但这在格雷厄姆老爷子那里简直是欺师灭祖，因为格老爷子说：安全边际靠啥？就是靠你支付的成本。对于任何证券而言，在某一个价格上也许有安全边际，但在另一个更高的价格上安全边际可能就烟消云散了。所以这事全看你买入的价格，成本就是你安全边际的命根儿。霍华德·马克斯（HowardMarks）也说过类似的话，“低价是防止出错的边际的最终来源”（“Lowpriceistheultimatesourceofmarginforerror”）。那什么样的资产可以提供安全边际呢？克拉曼认为首先必须是有形资产，他认为无形资产飘忽不定，弹性太大，如果出了啥幺蛾子，可能最后就屁都不值了。所以不能指望无形资产来提供安全边际，丫自己就不太安全。而反观有形资产则具有“替代用途”，比如你用一家店面来开个面店，结果跪了，你还可以转给别人开烧烤店；但是你原来“王大财拉面店”的品牌价值，如果找不到欣赏你的接盘侠，可能就一命呜呼了。于是谈安全边际，克神只看有形资产。所以克神在安全边际理论上又加了一些条件，这也是我目前所知道的对于此理论最为保守最为苛刻的解读，简直可以称其为“最安全边际”。他说啥叫安全边际啊，就是以该生意最低内在价值的巨大折扣价去投资该生意，并且偏爱于有形资产；然后我们不指望这笔生意经营得有多风生水起，哪怕它此时此刻就在我眼前被清算了，我也能够从中赚到钱。读到这里你肯定会微微一笑：别逗了怎么可能有这种好事？真有，只要你会找，只要你能等。双向情感障碍的Mr.市场会有极度抑郁的时期，尤其是对待一些人神共愤的扑街公司，简直有不鞭尸就浑身难受的变态情结。克神举例说在八十年代有一家叫ErieLackawanna的公司，而该公司当时创建的目的是为了清算掉在1976年就已经停止营运的ErieLackawanna铁路公司。至1987年年末，当时ErieLackawanna的股价大概交易在110美元的位置，但是公司账面上却有每股140美元的现金，另外还有向美国国税局索要巨额退税的请求权。即使不算退税，140美元的现金也已经超过了当时的股价，投资该公司的向下风险几乎为零，无非最多就是短期内会有浮亏。而到了1991年中旬，该公司累计向股东清算派发了179美元的现金，派发后剩余的资产仍以每股8美元在交易。当然这种投资机会当然如同凤毛麟角，所以克神常常持有大量现金，随随便便都是三到五成现金，没机会就不出手。这真正是做到知行合一的一个好榜样，除了佩服，我没有其他感受。一定要警惕伪价投题咱事先说明啊不做任何道德判断，伪价投不是伪君子。伪价投不一定是故意伪的，有可能你也真心想价投，但是理解有偏差或者能力跟不上，结果没投对；也有可能号称自己是价投，但是打心眼里其实根本不相信那一套，并不行价投之事。但是这些都没啥，无关乎道德，最多就亏点自己的钱呗；最可耻的是那些以价投作为幌子来吸引资产，而其投资哲学事实上与价投没有半毛钱关系的基金经理，克神给他们贴了一个标签——投资变色龙；骂完了他想想觉得意犹未尽，于是又多贴了一张标签——江湖老神棍。伪价投的神棍们利用八十年代价投大师们的声名鹊起——诸如巴菲特、MichaelPrice等人逐渐家喻户晓——而变色为价值投资者来招商引资。他们并不是骨子里的价投践行者，有些人可能自己也没弄明白到底啥是价投；还有一些更可恨，不管自己懂不懂，反正他们知道客户们大概率都不懂，于是他们利用大众对价投的片面理解来饱自己的钱囊。明面上为价投，实则是收人头。比如他们可能追涨某个市盈率已到五位数的IT股票，他说我这就是价投，你看未来二十一世纪就是电脑的天下，IT业多么有价值啊，你听我的，投这个股票就是投资价值。再比如有些哥们专买下跌股，格雷厄姆不是说了嘛安全边际要靠成本来实现，追涨是韭菜，那追跌我总是价投了吧？但是买下跌股与买便宜股之间还差着一个叶荣添与巴菲特的距离（Tobuythedipisnottobuythecheap）。专拣一泻千里的困境股越跌越买，这不是价投，这是断头。还有一些哥们那就是卷起袖子直接抄作业了啊，巴菲特买什么我就买什么，这总算是价投了吧？我就呵呵了，你这最多算资产托管啊北鼻。这些哥们显然都是伪到不行的价投，你可能会问，达某你在这里说几句风凉话难道就能独善其身了吗？我今天必须向大家来忏悔，我常说自己“大概持价投理念”（我不太敢直接以价投自居）；但是我要坦白，我可能就是一个伪价投。当夜深人静之时我自问每次交易都谨遵安全边际了吗，答案当然是一个大写的不，这尼玛我要是都能做到岂不是成了自虐狂的修道士了？没错，真正的价投就是对自己无比严苛的修道士，而我自问还远远还做不到——大概是因为尘世的喧嚣与内心的浮躁，或者大概是因为我没有那个基因。所以我不敢说自己是一个价投。但是我并不惭愧啊，这个世界上说自己是价投的百分之九十九都是伪价投；你如果认为自己是一个价投，那百分之九十九你也是伪价投。这话听着虽然刺耳，但是百分之九十九是正确的判断。也许看到这里你马上就不服了，我们不妨来做一个思维实验吧。纯粹假设啊：你的某个高高在上的女神股——咱比如说茅台吧——出于某种非黑天鹅的原因，一个礼拜连续五个跌停板，暴跌了40%，你买不？你会说卧槽这不废话吗我当然买啦。于是你是伪价投，因为咱说好的“内在价值”去哪了？你说不啊我搞过估值了茅台每股内在价值220块啊，现在跌到210块了，我当然买啦。于是你还是伪价投，因为咱说好的“最低的内在价值”去哪了？你说不啊我说错了我搞过估值了茅台的内在价值是220块-250块之间，所以我买啦。于是你仍然是伪价投，因为咱说好的“安全边际”去哪了？你估值茅台220-250，你给自己一个40%的安全边际，于是乎茅台跌到132块你才考虑去买，这才是真价投。你说这怎么可能呐，茅台跌到132块你是在做梦吗？道理就在这里，如果跌不到132块你就别去投资茅台呗，世界上那么多的股票，哪个能跌到，你再去买哪个。如果这个世界一片歌舞升平实在是找不到这样的投资标的，那你就拿着现金呗，这才是真正的价投，而最难能可贵的其实就是在这种时候能够“忍得住”。而这样的真价投的基金经理，恕鄙人见识浅陋，目前只见过赛斯·克拉曼一个；就连巴菲特大神，我都没有十足把握说他是一个这样的教科书一般的价值投资者。我自问反正我自己是肯定做不到如此去投资的，所以我不敢说自己是价投，我只能是“大概上持价投理念”；而咱也不要总是把“安全边际”当做一个投资术语放在嘴上把玩，真正能做到安全边际的，自古而今，又有几人？P.S.到二月我刚好来雪球一整年了，感谢大家也感谢雪球一年以来对我的支持与错爱。如无意外，只要大家愿意看，我就愿意继续写，为了我们大家热爱的投资事业。利益披露：作者在文章发表时不持有文中提到任何股票的仓位。本文行文仓莽，如有不足之处，还请各位海涵斧正。转载自：雪球，作者：陈达美股投资返回搜狐，查看更多责任编辑：',
 'edit': '',
 'link': 'http://www.sohu.com/a/200854533_143019',
 'source': '',
 'time': '2017-10-28 19:04',
 'title': '从Seth Klarman说起，告诉你真正的安全边际',
 'type': 'www.sohu.com'}
2017-10-28 22:41:41,73  [baidu_finance] ERROR: <twisted.python.failure.Failure zlib.error: Error -3 while decompressing data: invalid distance too far back>
2017-10-28 22:41:41,73  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure zlib.error: Error -3 while decompressing data: invalid distance too far back>
2017-10-28 22:41:57,107  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  User timeout caused connection failure: Getting http://news.qq.com/a/20171028/027677.htm took longer than 30.0 seconds..
<GET http://news.qq.com/a/20171028/027677.htm>
2017-10-28 22:41:57,108  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://news.qq.com/a/20171028/027677.htm> (failed 1 times): User timeout caused connection failure: Getting http://news.qq.com/a/20171028/027677.htm took longer than 30.0 seconds..
2017-10-28 22:41:57,143  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://news.qq.com/a/20171028/027677.htm> from <GET http://news.qq.com/a/20171028/027677.htm>
2017-10-28 22:42:00,103  [scrapy.core.engine] DEBUG: Crawled (200) <GET https://news.qq.com/a/20171028/027677.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:42:00,209  [scrapy.core.engine] INFO: Closing spider (finished)
2017-10-28 22:42:00,210  [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 10989,
 'downloader/request_count': 27,
 'downloader/request_method_count/GET': 27,
 'downloader/response_bytes': 417959,
 'downloader/response_count': 26,
 'downloader/response_status_count/200': 22,
 'downloader/response_status_count/301': 3,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 10, 28, 14, 42, 0, 209924),
 'item_scraped_count': 2,
 'log_count/DEBUG': 29,
 'log_count/ERROR': 10,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 21,
 'scheduler/dequeued': 27,
 'scheduler/dequeued/memory': 27,
 'scheduler/enqueued': 27,
 'scheduler/enqueued/memory': 27,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2017, 10, 28, 14, 41, 26, 833015)}
2017-10-28 22:42:00,211  [scrapy.core.engine] INFO: Spider closed (finished)
2017-10-28 22:44:34,262  [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: yixun)
2017-10-28 22:44:34,262  [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'yixun', 'CONCURRENT_REQUESTS': 1000, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_TIMEOUT': 50, 'LOG_FILE': 'log/20171028/22_baidu_finance_worker.log', 'LOG_FORMAT': '%(asctime)s,%(msecs)d  [%(name)s] %(levelname)s: %(message)s', 'NEWSPIDER_MODULE': 'yixun.spiders', 'RETRY_HTTP_CODES': [500, 502, 503, 504, 400, 408, 404, 403], 'RETRY_TIMES': 5, 'SPIDER_MODULES': ['yixun.spiders']}
2017-10-28 22:44:34,308  [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-10-28 22:44:34,779  [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'yixun.middlewares.ChoiceAgent',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'yixun.middlewares.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-10-28 22:44:34,782  [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'yixun.middlewares.ProcessResponseMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-10-28 22:44:34,783  [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-10-28 22:44:34,783  [scrapy.core.engine] INFO: Spider opened
2017-10-28 22:44:34,791  [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-10-28 22:44:34,792  [baidu_finance] INFO: Spider opened: baidu_finance
2017-10-28 22:44:34,794  [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024
2017-10-28 22:44:34,878  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.baidu.com/finance> (referer: None)
2017-10-28 22:44:35,64  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://business.sohu.com/20171028/n520469270.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,93  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.stockstar.com/SS2017102800002270.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,96  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.ifeng.com/a/20171028/52834010_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,100  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://industry.people.com.cn/n1/2017/1028/c413883-29614276.html> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,149  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.cecb2b.com/info/20171028/3613657.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,156  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://news.chinabyte.com/265/14329265.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,160  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://biz.jrj.com.cn/2017/10/28191423299779.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,203  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/cj/2017/10-28/8362769.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,215  [scrapy.core.scraper] DEBUG: Scraped from <200 http://finance.stockstar.com/SS2017102800002270.shtml>
{'about': 'from baidu',
 'author': '',
 'content': '（原标题：江苏：到2020年粮食生产基本实现全程机械化）摘要【江苏：到2020年粮食生产基本实现全程机械化】江苏省财政今年新增投入1.1亿元专项用于省级粮食生产全程机械化示范县建设，地方财政累计配套投入7.9亿元，其中34个示范县地方财政投入配套资金4.74亿元。江苏粮食生产全程机械化六大环节中，机耕、机收和秸秆机械化还田水平继续保持高位，薄弱环节机械化水平实现稳定提升，预计到2020年，粮食生产基本实现全程机械化。(新华社)江苏省财政今年新增投入1.1亿元专项用于省级粮食生产全程机械化示范县建设，地方财政累计配套投入7.9亿元，其中34个示范县地方财政投入配套资金4.74亿元。江苏粮食生产全程机械化六大环节中，机耕、机收和秸秆机械化还田水平继续保持高位，薄弱环节机械化水平实现稳定提升，预计到2020年，粮食生产基本实现全程机械化。',
 'edit': '',
 'link': 'http://finance.stockstar.com/SS2017102800002270.shtml',
 'source': '新华社',
 'time': '2017-10-28 20:13:13',
 'title': '江苏：到2020年粮食生产基本实现全程机械化',
 'type': 'finance.stockstar.com'}
2017-10-28 22:44:35,283  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.ifeng.com/a/20171028/15751804_0.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,292  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362753.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,336  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28205123299849.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,390  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350864> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:35,437  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72887: illegal multibyte sequence>
2017-10-28 22:44:35,437  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 72887: illegal multibyte sequence>
2017-10-28 22:44:37,139  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.chinanews.com/cj/2017/10-28/8362715.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:37,501  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.licai18.com/article/ArticleDetail.jsp?docId=2350881> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:37,616  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]
<GET http://www.chinanews.com/ty/shipin/cns-d/2017/10-28/news738503.shtml>
2017-10-28 22:44:37,616  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://www.chinanews.com/ty/shipin/cns-d/2017/10-28/news738503.shtml> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]
2017-10-28 22:44:37,687  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200851917_162818> from <GET http://business.sohu.com/20171028/n520449743.shtml>
2017-10-28 22:44:38,29  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28211923299869.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:38,130  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70591: illegal multibyte sequence>
2017-10-28 22:44:38,130  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70591: illegal multibyte sequence>
2017-10-28 22:44:39,384  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://finance.jrj.com.cn/2017/10/28212023299870.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:39,486  [baidu_finance] ERROR: <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70400: illegal multibyte sequence>
2017-10-28 22:44:39,486  [baidu_finance] ERROR: 反爬/超时/其他错误 <twisted.python.failure.Failure builtins.UnicodeDecodeError: 'gbk' codec can't decode byte 0xa1 in position 70400: illegal multibyte sequence>
2017-10-28 22:44:40,117  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.chinanews.com/ty/shipin/cns-d/2017/10-28/news738503.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:40,265  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.sohu.com/a/200854533_143019> from <GET http://business.sohu.com/20171028/n520452220.shtml>
2017-10-28 22:44:43,42  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://business.sohu.com/20171028/n520452931.shtml> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:44,727  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200851917_162818> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:44,857  [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.sohu.com/a/200851917_162818>
{'about': 'from baidu',
 'author': '',
 'content': '随着深圳终于后知后觉地袭来阵阵凉意，2017年也即将敲响结束的钟声。作为一名周期行业的研究员，笔者在今年也是见证了熄火许久的周期行业都各自迎来自己的大牛市，从造纸、钢铁、水泥、煤炭到有色，相信大多数追随周期的投资者都已经获得了不错的收益。前几日，笔者听闻一个朋友，他在年中的时候买入江西铜业（00358.HK），理由是看好铜价的走势，所以买一点铜业的龙头。结果是，铜价今年确实也走的不错，LME铜上周刚刚突破7000美元大关，刷新近三年的新高（全年走势见上图）。铜价今年的回暖涨幅，也并不比相似属性的铝或者钢材差太多。尤其是LME铜，年初至今累计涨幅将近27%，并且发力主要集中在下半年，近两个月累计涨幅约10%。但是，相对于其它周期股，铜股的走势也就只能用“虚弱”来形容了，朋友表示十分郁闷，好像商品的价格对股价影响并不明显。港股的铜概念股代表，还有一家紫金矿业（02899.HK），涨幅同样很微弱。再看看周期其他行业的龙头，随着商品价格的上涨，股价也基本一起“升天”了。那么，问题就来了。面对亮眼的铜价，以及持续平庸的股价，铜股还值得我们继续坚守吗？要回答这个问题，首先需要明确铜股股价不太涨的原因。笔者认为，导致的原因可能主要有两点。▌一、原因一：市场对铜价进一步上涨缺乏自信股市是经济的晴雨表，股票价格反映的也是市场对未来价值的预期。铜价上涨，而股价不涨，很大可能就是市场对后市判断缺少基准，拿不准铜价未来的走势，所以对铜价进一步上涨的空间缺乏自信，保持谨慎态度。前面提到的，今年股价势如破竹的板块，其实大多数都有两个相同的特征。特征一是企业绝大多数产能都集中在国内，例如电解铝，例如钢材，再例如水泥等等。特征二则是行业都处在国家主动的调控下，例如供给侧改革，例如环保督查关闭不合规小厂，再例如北方采暖季停工等等，这些政策的最终导向，都是很明确的产能减少。并且，今年的政策力度还非常大。这些动作，都给了明确的“强心剂”，让市场对行业的后市充满信心。但是对于铜来说，则不太具备以上两个特征。中国是世界上最大的铜消费国，但是铜的储量和基础储量分别仅占世界总量的5.53%和6.67%，对于铜精矿的需求，需要依靠大量从智利、秘鲁、澳洲等地进口来满足。从2003年到2014年，中国铜精矿进口量，从266.7万吨增加到1182万吨。由于产能不在国内，也就沾不到国内大力度去产能的利好，市场对于后市的判断比较模糊，也就能够解释。▌二、原因二：铜企的盈利尚未释放铜价上升通道开启的比较晚，基本是在七月底。而商品价格变化带给公司的盈利，很多并不会立刻释放。虽然有客观的原因导致铜股的股价走势并不理想。但是笔者认为，以上两个原因，对铜企股价释放的阻力，未来有机会逐渐减小。▌三、铜价走势已经基本明朗化首先，铜价未来的走势已经从模糊步入明朗化。今年铜价的上涨，主要是由于商品库存的下降以及全球经济的企稳。目前，全球的货币政策仍趋紧，这意味着全球经济比预期要更强劲。铜作为基本金属，其价格很大程度上跟经济挂钩。所以从大体经济上来看，铜基本可以保持一个长牛的走势。10月24日，高盛也将未来１２个月的LME铜目标价上调至７０５０美元，该行预计随着经济的修复，明年铜的供应将会短缺１３万吨。1.供给端铜精矿加工费（TC/RC），是将铜精矿转化为精铜的处理和精炼费用，即矿产商和贸易商向冶炼厂支付的费用。TC/RC可以很好的反映铜精矿的供给，因为决定加工费高低的因素，就是铜精矿的供求关系。当铜精矿供给短缺时，矿产主对冶炼厂商的谈判占据主动，其支付的费用就会降低。反之，费用则上升。截至2017年9月，TC/RC费用已经从2016年11月的101.5/10.15美元/吨，降低到了83/8.3美元/吨，这正面说明了铜精矿供给量正在下降的事实。查阅数据，上半年，全球的矿山产量只有1月份同比增加了，2-6月份同比分别下滑5%、7%、0%、2%和2%。且接下来，铜精矿的供给仍然会受到限制。出口受限：据悉，刚果（金）方面要求华刚矿业在国内将铜和钴加工成金属再出口，不允许出口未经加工的铜和钴。今年上半年，华钢矿业供给出口了11.5万吨铜精矿，年产量为23万吨，计划产能40万吨。华钢矿业出口的铜精矿量虽不多，但是目前全球的铜供给处于紧平衡或者存在缺口，该限制，可能进一步恶化铜精矿的供给情况。劳工协议到期：今年10月20日到11月30日，据统计有约92万吨产能的劳工协议到期，且大部分位于智利，有可能再度引发上半年的劳工罢工的事件。短期新增产能不多：一般来说，铜矿山从资本支出到正式投产，需要4-5年的时间，投产周期较长。2011年，全球30家主要铜矿公司支出资本达到峰值，之后支出明显下滑，新增的铜矿产能也相应地在2015-2016年达到峰值。最近一波的全球铜矿产能投资，基本是集中于2016年铜价回暖之后，所以产能高峰产出算一算也需要在2022年左右才会到来。据悉全球十大铜企，只有力拓和必和必拓在2020年前有新增产能投入，其余公司均没有计划。2.需求端中国基础设施建设：截至2017年8月，我国基础设施建设投资和公共财政支出分别同比增长16.09%和13.08%。增速虽稍有放缓，但是依然维持在两位数。美国基建：特朗普表示，未来五年对基建投资将增加5500亿美元。如若顺利实施，美国2017-2020每年基建投资的增长率将达到5.7%。根据高盛的预测，特朗普每年约有1万亿美元的基建项目，在未来的10年内将推动美国铜消费增速上升至4%。“一带一路”：沿线国家的建设薄弱，也有望成为拉动铜需求的新亮点。国内铜需求平稳：我国铜终端的需求主要是集中在电力、家电（空调为主）、交通和建筑领域。截止8月份，电网基本建设投资累计完成额同比增长7.88%，且“十三五”规划电网投资3.34万亿，是“十二五”2万亿规划的1.67倍，电网建设的投资会加倍增长；截止8月份，我国汽车产量累计增长5.9%，由于新能源汽车的耗铜量是传统能源汽车的两倍，新能源汽车对铜的需求也有拉动作用。如果全球年产500万辆的新能源汽车，将增加铜需求25万吨；同样截止8月份，房地产新开工面积和房地产开发投资完成额同比增长7.6%和7.9%，略有下滑，但是二、三线城市地产去库存时间逐渐缩短，预计未来也不会大幅下滑。▌四、盈利会随着铜价稳企逐渐释放金属的价格周期表现分为两类，一类是倒“Ｖ”走势，也就是“怎么上去，怎么下来”；还有一类则是“上去，横盘，再下跌”，这也是金属市场最喜闻乐见但是不多见的玩法。因为随着横盘，金属价格能够在较高位保持一段时间，这也就有利于企业的滞后的盈利逐渐释放，带动股价暴涨。下面，笔者举两个历史上的栗子。第一个也就是发生在铜行业的栗子，２００２年初到２００６年５月，ＬＭＥ铜一度从２０００美元／吨上涨到８０００美元／吨，而江西铜业的Ｈ股股价也从不到１港元涨到７港元。接下来的２００６年５月到２００７年１０月，ＬＭＥ铜基本是震荡维持在了８０００美元／吨左右的高位，随着盈利的逐步释放，江西铜业的股价也就从７港元飙升到了３２港元，涨幅达到约３５０％。第二个是发生在碳酸锂行业的栗子，２０１４年底碳酸锂价格从４万／吨上涨到２０１６年５月的１９万／吨，这期间天齐锂业（００２４６６.ｓｚ）的股价涨幅达４００％。之后随着碳酸锂价格的高位稳企，盈利逐步释放，２０１７年天齐锂业的股价继续上涨约２００％。目前，铜价已经是处在近三年来的一个高位。前文也已经说到，铜价的后市基本比较明朗，那么铜企在套期保值上大亏损的概率也就降低。再加上供需关系收紧，那么铜价能不能保持在一个较高位的价格，带动铜股的爆发，也就值得期待了。▌五、投资机会江西铜业，是集采矿、选矿、冶炼、加工、贸易、技术为一体的国内最大的综合性铜生产企业之一，旗下子公司众多，贯通铜行业产业链条。控股的主要矿山有德兴铜矿、永平铜矿、城门山铜矿、武山铜矿、东乡铜矿以及银山铅锌矿，并且拥有国内规模最大的铜冶炼厂－贵溪冶炼厂。截止2016年底，江西铜业铜金属资源总量983万吨，权益铜资源储量为443万吨，铜精矿含铜产能２１万吨／年；除此之外，公司的财务状况良好，目前的资产负债率仅为48.20％，远低于国内铜行业其他公司水平。紫金矿业，虽然看名字是金概念股，但是近几年其铜业务的营收已经逐渐取代金矿业务。且公司极具有商业投资嗅觉，在前几年大宗商品还处于低迷的时期，紫金矿业就陆续收购了澳洲亚诺顿金矿、刚果（金）科卢齐铜矿、黑龙江多宝山铜矿等大型项目，并且合作开发巴新波格拉金矿以及刚果（金）卡莫阿铜矿。紫金矿业的铜资源储量也非常高，截止2016年底，公司拥有铜资源储量3006万吨，约为江西铜业储量的2.1倍，占全国总资源的将近三分之一。看估值，江西铜业目前的市净率是32.5倍。而紫金矿业的市净率为20.2，相应高的铜的资源，以及较低的估值，待铜价稳企之时，可能会具有更高的弹性。当然，行业若转好，两个龙头都不会差。▌小结综合来看，铜股股价能不能爆发的关键，也就是这周期铜价的走势，到底能不能具备“上去，横盘，再下来”的特征。经济的稳企，以及供需关系的收紧，接下来关于铜的故事，让我们拭目以待~【作者简介】抹茶拿铁|格隆汇·专栏作者长期关注周期行业的二级狗一枚爱阅读爱思考，耐心等待机会返回搜狐，查看更多责任编辑：',
 'edit': '',
 'link': 'http://www.sohu.com/a/200851917_162818',
 'source': '',
 'time': '2017-10-28 19:23',
 'title': '为什么铜价持续上涨，铜股却表现平庸？',
 'type': 'www.sohu.com'}
2017-10-28 22:44:47,604  [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.sohu.com/a/200854533_143019> (referer: http://news.baidu.com/finance)
2017-10-28 22:44:47,735  [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.sohu.com/a/200854533_143019>
{'about': 'from baidu',
 'author': '',
 'content': '文|陈达导读：今天想来介绍一位我佩服至极的低调大神——对冲基金经理赛斯·克拉曼（SethKlarman）。他和他的对冲基金Baupost妖娆的业绩神话我就不多废话了，有兴趣的可以参见拙作：《像对冲基金经理那样去思考：事件驱动交易以及扯一点关于价投的淡》。本文仅仅想对他的一些投资思想与哲学做一个简单梳理。当然其思想体系波大精深，我也只是说个部分，如果以后有机会，我们再来狗尾续丑雕。从一个关于投资的小故事说起题在遥远的加利福尼亚共和国，有一个美如画的海边小镇叫蒙特雷（Monterey），其附近海域在一百年前曾是沙丁鱼扎堆的大渔场。于是靠海吃海，小镇的渔业与罐头食品业生意昌隆。然而天有不测风云，出于某种不可解释的神奇力量，二战以后那里的沙丁鱼纷纷神秘消失，但岸边上的食品加工厂都是有很高的沉没成本的呀，机器厂房哪能闲置，含着泪也要把工开；于是沙丁鱼价格被炒，沙丁鱼罐头的价格也随之旱地拔葱。如同古人炒郁金香今人炒大红袍，这沙丁鱼罐头上来就是一波恶炒。某天有一个炒家一时兴起打算对自己好一点，于是晚饭就开了一个巨贵无比的沙丁鱼罐头来吃，以示哥们我就是这么土豪。结果该罐头已变质，哥们差点挂掉。于是他找到罐头的卖家理论，说你这罐头有质量问题。结果卖家斜瞄了他一眼，幽幽地说：你这个瓜娃子，这个是交易罐头，不是用来吃的。所以也有一段时间郁金香不是拿来种的，大红袍不是拿来喝的，他们都是交易花和交易茶，你要是傻兮兮地拿来泡茶，只能落得人丑茶凉的结局。投机炒客对于买卖决定的判断来自于对其他交易对手行为的预测；但根据克大神，一个克己的投资者，他们收益的着眼点应该是：一、从某一门生意中涓涓而来的自由现金流，最终要么反映为更高的股价，要么反映为分红股利；二、从其他投资者愿意支付的溢价，反映为更高的估值乘数；三、从股价与内在价值的鸿沟的不断的坍塌缩短而来。克神谈价投题在为格雷厄姆老爷子的《证券投资》（SecurityAnalysis）第六版所做的序言里，克拉曼谈了谈他眼中的价值投资：啥叫价投，是以资产的价值以下的价格进行投资，留出足够的安全边际。而安全边际的使命，是在你的血汗钱与你的失算、出错、厄运和经济与市场本身的风云诡谲之间搭起缓冲区域。很多人认为价投就是“找个低价来投”，这就有点肤浅，价投是一套哲学小宇宙，包括了深度的基本面分析、追求长期投资目标、风险控制与拒绝从众等等漫天漫宇的小星星。如果认为价投就是“找个低价来投”，那么你有可能，低价买了个破产货出局（不做基本面分析）；低价买了但是小亏止损出局（没有长期持股的目标）；低价融资壕一把结果爆仓出局（无风控意识）；低价买了结果在一片唱衰声中黯然出局（顶不住群众大流的压力与背离人群的恐惧）。克神的另外一个观点就比较极端了，容易让人感觉受到了侮辱：他认为价投所需要的人格特质——无论是沉静、耐性、纪律和反风险——都是基因决定的。所以据此观点，你是不是一个价投在爹妈当年擦出的那一次火花中就已经尘埃落定了，你要么天生就是价投，要么永远都不会是。他认为，你听到价投的理念，要么你深深地发出共鸣，要么觉得这他妈是什么**玩意儿，这就是你基因深处的一次理性与情感的喷薄。类似的观点巴菲特也曾经感慨过。在《格雷厄姆-多德村的超级投资家》（TheSuperinvestorsofGraham-and-Doddsville）中巴老表示：（价值投资）花四毛钱买一块钱资产的这个念头，人们要么是秒懂，要么是永远都不懂；这就跟移花接木一样，要么有些直接就参天大树，剩下的那些人你就算传道一百年、拿着各种证据啊记录啊摊开在他面前，也是鸡同鸭讲，不同世界的人没必要强行沟通。按照这个观点，我们不用担心全市场都会变成夹头从而消弭价值投资的超额收益，或者担心雪球被夹头们沦陷，因为这不可能嘛——一部分人”从基因上不可能”成为夹头。最安全边际题有人问巴菲特，投资里有没有四字真言送给我们这些韭菜丫？这其实是个韭菜问题，但巴菲特还真就想了想，郑重其事地回答说：有，送你四个字：安全边际。他认为这是投资里最重要的四个汉字（或三个单词marginofsafety）。于是我们就听到了铺天盖地对“安全边际”的喧哗与骚动。我知道对于很多人而言这玩意儿都要听得都恶心了，但我也知道对于很多人而言未必知道这个四字真言的确切含义。而克神比巴老对于安全边际要更加执念一点，他不仅嘴上说安全边际重要，还特意写了一本目前市面上价格三千美金的书，来彻底阐述安全边际的重要性，并将该书取名为——请原谅理科生的直男思维——《安全边际》。安全边际一词，第一次出现是在格雷厄姆祖宗的《证券分析》一书里，原文定义是股票现价与它的最低的内在价值之间的折扣。可能有人会说不对啊这里有个逻辑问题，市场为啥要打折卖这不是缺心眼吗？这就回到了格雷厄姆关于Mr.Market的那个隐喻，Mr.Market是典型的双向情感障碍（bipolar）患者，也就是同时罹患狂躁症与抑郁症的病人。在狂躁的时候他兴高采烈地溢价买卖，在抑郁的时候他心灰意冷地折价买卖，所以就会有打折的可能，就有出现安全边际的可能。说起来也许略抽象，画个图来巩固一下。如果你相信市场是完全有效的而不是一个双向情感障碍患者，那么安全边际也不要去想了，因为对你而言它并不存在。从图中来看这个安全边际的概念看似简单朴素得喜闻乐见，其实略微还是有点深意的。第一，内在价值是“最低的内在价值”（minimumintrinsicvalue），有人可能会问奇怪了内在价值为啥还有最高最低，它不就是一数字么？其实不然，因为你去问任何夹头大神，无论是巴菲特还是克拉曼甚至是格雷厄姆，你去问他们000932的内在价值是多少，他们一定不会告诉你一个精确数字，这个只有神棍才能做到。估值精确到一个点，统计上这叫点估计，统计学告诉我们点估计都是不靠谱的，所以我们的内在价值一定是一个大概的模糊的区间。做过比较深度的估值练*的同学们都知道最后出来的结果一般而言就是一个区间——比如我算出来000932的内在价值大概是在1块钱到20块钱之间——因为我估值不可能只用一种方法来估，如果你通过不同的估值方法却纷纷做出来同样一个结果，那就是无耻的巧合，我们会怀疑你是不是在搞反向工程。想要不无耻你就只能给区间，因为那么多个结果谁对谁错你其实并无把握；不耍流氓的天气预报一定给的是降雨概率，而不耍流氓的估值分析一定给的是价格区间。古谚有云，含糊的正确要好过精准的错误。我们对内在价值的估计是含糊的，所以就存在最低的内在价值，而安全边际——至少在格老爷子的心中——是以最低的内在价值为衣钵的。在最低的内在价值上打出的折扣才是安全边际。格老爷子的意思是，我认为000932的内在价值是5块，但我徒弟巴菲特认为值4块，而他的好基友芒格认为只值3块，好吧爱徒及其基友的话我是要听的，所以我综合认为000932最低的内在价值就是3快；而现在000932正好卖3块，我买不买呢？当然不买，值3块的东西我花3块买岂不是在蹉跎青春，那必须再打个六折我才会去考虑——其中的40%off就是安全边际，六折以下的价格就是买点。而巴菲特对此的理解是另一个鲜活的隐喻：我建一座能承重三万磅的桥，但却只让通一万磅的车；这样就算我大意了失算了漏放了一辆一万两千磅或者一万三千磅的卡车过去，也不至于桥毁人亡。所以上面的那一个插图，就可以进一步演化为：这是价值投资中安全边际的完整含义。很多人说价投是不管成本的，要与持股成本相忘于江湖；但这在格雷厄姆老爷子那里简直是欺师灭祖，因为格老爷子说：安全边际靠啥？就是靠你支付的成本。对于任何证券而言，在某一个价格上也许有安全边际，但在另一个更高的价格上安全边际可能就烟消云散了。所以这事全看你买入的价格，成本就是你安全边际的命根儿。霍华德·马克斯（HowardMarks）也说过类似的话，“低价是防止出错的边际的最终来源”（“Lowpriceistheultimatesourceofmarginforerror”）。那什么样的资产可以提供安全边际呢？克拉曼认为首先必须是有形资产，他认为无形资产飘忽不定，弹性太大，如果出了啥幺蛾子，可能最后就屁都不值了。所以不能指望无形资产来提供安全边际，丫自己就不太安全。而反观有形资产则具有“替代用途”，比如你用一家店面来开个面店，结果跪了，你还可以转给别人开烧烤店；但是你原来“王大财拉面店”的品牌价值，如果找不到欣赏你的接盘侠，可能就一命呜呼了。于是谈安全边际，克神只看有形资产。所以克神在安全边际理论上又加了一些条件，这也是我目前所知道的对于此理论最为保守最为苛刻的解读，简直可以称其为“最安全边际”。他说啥叫安全边际啊，就是以该生意最低内在价值的巨大折扣价去投资该生意，并且偏爱于有形资产；然后我们不指望这笔生意经营得有多风生水起，哪怕它此时此刻就在我眼前被清算了，我也能够从中赚到钱。读到这里你肯定会微微一笑：别逗了怎么可能有这种好事？真有，只要你会找，只要你能等。双向情感障碍的Mr.市场会有极度抑郁的时期，尤其是对待一些人神共愤的扑街公司，简直有不鞭尸就浑身难受的变态情结。克神举例说在八十年代有一家叫ErieLackawanna的公司，而该公司当时创建的目的是为了清算掉在1976年就已经停止营运的ErieLackawanna铁路公司。至1987年年末，当时ErieLackawanna的股价大概交易在110美元的位置，但是公司账面上却有每股140美元的现金，另外还有向美国国税局索要巨额退税的请求权。即使不算退税，140美元的现金也已经超过了当时的股价，投资该公司的向下风险几乎为零，无非最多就是短期内会有浮亏。而到了1991年中旬，该公司累计向股东清算派发了179美元的现金，派发后剩余的资产仍以每股8美元在交易。当然这种投资机会当然如同凤毛麟角，所以克神常常持有大量现金，随随便便都是三到五成现金，没机会就不出手。这真正是做到知行合一的一个好榜样，除了佩服，我没有其他感受。一定要警惕伪价投题咱事先说明啊不做任何道德判断，伪价投不是伪君子。伪价投不一定是故意伪的，有可能你也真心想价投，但是理解有偏差或者能力跟不上，结果没投对；也有可能号称自己是价投，但是打心眼里其实根本不相信那一套，并不行价投之事。但是这些都没啥，无关乎道德，最多就亏点自己的钱呗；最可耻的是那些以价投作为幌子来吸引资产，而其投资哲学事实上与价投没有半毛钱关系的基金经理，克神给他们贴了一个标签——投资变色龙；骂完了他想想觉得意犹未尽，于是又多贴了一张标签——江湖老神棍。伪价投的神棍们利用八十年代价投大师们的声名鹊起——诸如巴菲特、MichaelPrice等人逐渐家喻户晓——而变色为价值投资者来招商引资。他们并不是骨子里的价投践行者，有些人可能自己也没弄明白到底啥是价投；还有一些更可恨，不管自己懂不懂，反正他们知道客户们大概率都不懂，于是他们利用大众对价投的片面理解来饱自己的钱囊。明面上为价投，实则是收人头。比如他们可能追涨某个市盈率已到五位数的IT股票，他说我这就是价投，你看未来二十一世纪就是电脑的天下，IT业多么有价值啊，你听我的，投这个股票就是投资价值。再比如有些哥们专买下跌股，格雷厄姆不是说了嘛安全边际要靠成本来实现，追涨是韭菜，那追跌我总是价投了吧？但是买下跌股与买便宜股之间还差着一个叶荣添与巴菲特的距离（Tobuythedipisnottobuythecheap）。专拣一泻千里的困境股越跌越买，这不是价投，这是断头。还有一些哥们那就是卷起袖子直接抄作业了啊，巴菲特买什么我就买什么，这总算是价投了吧？我就呵呵了，你这最多算资产托管啊北鼻。这些哥们显然都是伪到不行的价投，你可能会问，达某你在这里说几句风凉话难道就能独善其身了吗？我今天必须向大家来忏悔，我常说自己“大概持价投理念”（我不太敢直接以价投自居）；但是我要坦白，我可能就是一个伪价投。当夜深人静之时我自问每次交易都谨遵安全边际了吗，答案当然是一个大写的不，这尼玛我要是都能做到岂不是成了自虐狂的修道士了？没错，真正的价投就是对自己无比严苛的修道士，而我自问还远远还做不到——大概是因为尘世的喧嚣与内心的浮躁，或者大概是因为我没有那个基因。所以我不敢说自己是一个价投。但是我并不惭愧啊，这个世界上说自己是价投的百分之九十九都是伪价投；你如果认为自己是一个价投，那百分之九十九你也是伪价投。这话听着虽然刺耳，但是百分之九十九是正确的判断。也许看到这里你马上就不服了，我们不妨来做一个思维实验吧。纯粹假设啊：你的某个高高在上的女神股——咱比如说茅台吧——出于某种非黑天鹅的原因，一个礼拜连续五个跌停板，暴跌了40%，你买不？你会说卧槽这不废话吗我当然买啦。于是你是伪价投，因为咱说好的“内在价值”去哪了？你说不啊我搞过估值了茅台每股内在价值220块啊，现在跌到210块了，我当然买啦。于是你还是伪价投，因为咱说好的“最低的内在价值”去哪了？你说不啊我说错了我搞过估值了茅台的内在价值是220块-250块之间，所以我买啦。于是你仍然是伪价投，因为咱说好的“安全边际”去哪了？你估值茅台220-250，你给自己一个40%的安全边际，于是乎茅台跌到132块你才考虑去买，这才是真价投。你说这怎么可能呐，茅台跌到132块你是在做梦吗？道理就在这里，如果跌不到132块你就别去投资茅台呗，世界上那么多的股票，哪个能跌到，你再去买哪个。如果这个世界一片歌舞升平实在是找不到这样的投资标的，那你就拿着现金呗，这才是真正的价投，而最难能可贵的其实就是在这种时候能够“忍得住”。而这样的真价投的基金经理，恕鄙人见识浅陋，目前只见过赛斯·克拉曼一个；就连巴菲特大神，我都没有十足把握说他是一个这样的教科书一般的价值投资者。我自问反正我自己是肯定做不到如此去投资的，所以我不敢说自己是价投，我只能是“大概上持价投理念”；而咱也不要总是把“安全边际”当做一个投资术语放在嘴上把玩，真正能做到安全边际的，自古而今，又有几人？P.S.到二月我刚好来雪球一整年了，感谢大家也感谢雪球一年以来对我的支持与错爱。如无意外，只要大家愿意看，我就愿意继续写，为了我们大家热爱的投资事业。利益披露：作者在文章发表时不持有文中提到任何股票的仓位。本文行文仓莽，如有不足之处，还请各位海涵斧正。转载自：雪球，作者：陈达美股投资返回搜狐，查看更多责任编辑：',
 'edit': '',
 'link': 'http://www.sohu.com/a/200854533_143019',
 'source': '',
 'time': '2017-10-28 19:04',
 'title': '从Seth Klarman说起，告诉你真正的安全边际',
 'type': 'www.sohu.com'}
2017-10-28 22:45:05,26  [[E:/python_code/Spider3/distributed/yixun/single_main.py]] ERROR: 错误类型  User timeout caused connection failure: Getting http://news.qq.com/a/20171028/027677.htm took longer than 30.0 seconds..
<GET http://news.qq.com/a/20171028/027677.htm>
2017-10-28 22:45:05,27  [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://news.qq.com/a/20171028/027677.htm> (failed 1 times): User timeout caused connection failure: Getting http://news.qq.com/a/20171028/027677.htm took longer than 30.0 seconds..
2017-10-28 22:45:05,60  [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://news.qq.com/a/20171028/027677.htm> from <GET http://news.qq.com/a/20171028/027677.htm>
2017-10-28 22:45:13,94  [scrapy.core.engine] DEBUG: Crawled (200) <GET https://news.qq.com/a/20171028/027677.htm> (referer: http://news.baidu.com/finance)
2017-10-28 22:45:13,200  [scrapy.core.engine] INFO: Closing spider (finished)
2017-10-28 22:45:13,202  [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 11029,
 'downloader/request_count': 27,
 'downloader/request_method_count/GET': 27,
 'downloader/response_bytes': 418584,
 'downloader/response_count': 25,
 'downloader/response_status_count/200': 22,
 'downloader/response_status_count/301': 2,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 10, 28, 14, 45, 13, 200962),
 'item_scraped_count': 3,
 'log_count/DEBUG': 31,
 'log_count/ERROR': 8,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 22,
 'scheduler/dequeued': 27,
 'scheduler/dequeued/memory': 27,
 'scheduler/enqueued': 27,
 'scheduler/enqueued/memory': 27,
 'start_time': datetime.datetime(2017, 10, 28, 14, 44, 34, 791765)}
2017-10-28 22:45:13,202  [scrapy.core.engine] INFO: Spider closed (finished)
